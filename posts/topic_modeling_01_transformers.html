
<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Topic Modeling on Surface Trivia Question Dataset - r4z4 Site</title>
    <link rel="stylesheet" href="/assets/css/style.css">
    <link rel="stylesheet" href="/assets/css/home_animation.css">
    <link rel="stylesheet" href="/assets/css/markdown.css">
  </head>
  <body>
    
<div id="navbar_div">
  <ul id="navbar">
    <li><a href="/index.html">Home</a></li>
    <li><a href="/posts">Posts</a></li>
  </ul>
</div>

    <h1 class="base-header"><a href="/">Various Writings and Observations</a></h1>
    <section class="md-body"><h1 id="topic-modeling-on-surface-trivia-question-dataset">Topic Modeling on Surface Trivia Question Dataset</h1><p>Posted on Wednesday, 16 Aug 2023 by r4z4</p><p>Tags:</p><ul><li><a href="/tags/NLP">NLP</a></li><li><a href="/tags/text-classification">text-classification</a></li></ul><p>I have worked with transformers a bit in the past but did not really account for my progress or do any real comparisons. I will be using the dataset that I assembled for a trivia app in Elixir that I built. There are a handful of categories each with a good amount of questions, and we will see that the transformers approach makes the process much easier.</p><hr class="thin"/><pre><code class="python">import pandas as pd
import numpy as np
import json
import re</code></pre><pre><code class="python">df = pd.read_json(&quot;trivia_data.json&quot;)</code></pre><pre><code class="python">## Gonna take a while to embed all the data (&gt; 2hrs on CPU). Lets just use 20% of the data
# n = 20
# df = df.head(int(len(df)*(n/100)))

data = df[&#39;corrected_question&#39;]
data_array = np.array([e for e in df[&#39;corrected_question&#39;]])</code></pre><pre><code class="python">len(data)</code></pre><pre><code>4000</code></pre><pre><code class="python">from sentence_transformers import SentenceTransformer
model = SentenceTransformer(&#39;distilbert-base-nli-mean-tokens&#39;)
embeddings = model.encode(data, show_progress_bar=True)</code></pre><pre><code>Batches:   0%|          | 0/125 [00:00&lt;?, ?it/s]</code></pre><pre><code class="python">import umap.umap_ as umap
umap_embeddings = umap.UMAP(n_neighbors=15, 
                            n_components=5, 
                            metric=&#39;cosine&#39;).fit_transform(embeddings)</code></pre><pre><code class="python">import hdbscan
cluster = hdbscan.HDBSCAN(min_cluster_size=15,
                          metric=&#39;euclidean&#39;,                      
                          cluster_selection_method=&#39;eom&#39;).fit(umap_embeddings)</code></pre><pre><code class="python">import matplotlib.pyplot as plt

# Prepare data
umap_data = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric=&#39;cosine&#39;).fit_transform(embeddings)
result = pd.DataFrame(umap_data, columns=[&#39;x&#39;, &#39;y&#39;])
result[&#39;labels&#39;] = cluster.labels_

# Visualize clusters
fig, ax = plt.subplots(figsize=(20, 10))
outliers = result.loc[result.labels == -1, :]
clustered = result.loc[result.labels != -1, :]
plt.scatter(outliers.x, outliers.y, color=&#39;#BDBDBD&#39;, s=0.05)
plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=0.05, cmap=&#39;hsv_r&#39;)
plt.colorbar()</code></pre><p><img src="/assets/images/topic-modeling/01_transformers.png#img-thumbnail" alt="png"/></p><pre><code class="python">docs_df = pd.DataFrame(data_array, columns=[&quot;Doc&quot;])
docs_df[&#39;Topic&#39;] = cluster.labels_
docs_df[&#39;Doc_ID&#39;] = range(len(docs_df))
docs_per_topic = docs_df.groupby([&#39;Topic&#39;], as_index = False).agg({&#39;Doc&#39;: &#39; &#39;.join})</code></pre><pre><code class="python">docs_per_topic.head()</code></pre><pre><code class="python">docs_df[&#39;Topic&#39;].value_counts()</code></pre><pre><code>-1     1338
 44     203
 17     203
 21     194
 29     137
 19     102
 38      96
 11      95
 39      86
 9       75
 33      73
 13      71
 45      70
 14      70
 35      67
 4       66
 10      64
 41      64
 7       64
 12      56
 42      54
 43      47
 36      47
 15      44
 23      42
 5       40
 26      38
 30      36
 8       34
 24      34
 37      33
 6       29
 3       28
 27      28
 0       27
 34      27
 2       26
 32      25
 18      21
 20      20
 28      19
 1       19
 22      19
 31      18
 40      18
 16      17
 25      16
Name: Topic, dtype: int64</code></pre><pre><code class="python">docs_df.head()</code></pre><pre><code class="python">import numpy as np
from sklearn.feature_extraction.text import CountVectorizer

def c_tf_idf(documents, m, ngram_range=(1, 1)):
    count = CountVectorizer(ngram_range=ngram_range, stop_words=&quot;english&quot;).fit(documents)
    t = count.transform(documents).toarray()
    w = t.sum(axis=1)
    tf = np.divide(t.T, w)
    sum_t = t.sum(axis=0)
    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)
    tf_idf = np.multiply(tf, idf)

    return tf_idf, count
  
tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m=len(data))</code></pre><pre><code class="python">def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):
    words = count.get_feature_names_out()
    labels = list(docs_per_topic.Topic)
    tf_idf_transposed = tf_idf.T
    indices = tf_idf_transposed.argsort()[:, -n:]
    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}
    return top_n_words

def extract_topic_sizes(df):
    topic_sizes = (df.groupby([&#39;Topic&#39;])
                     .Doc
                     .count()
                     .reset_index()
                     .rename({&quot;Topic&quot;: &quot;Topic&quot;, &quot;Doc&quot;: &quot;Size&quot;}, axis=&#39;columns&#39;)
                     .sort_values(&quot;Size&quot;, ascending=False))
    return topic_sizes

top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)
topic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(10)</code></pre><pre><code class="python">top_n_words[21][:10]</code></pre><pre><code>[(&#39;television&#39;, 0.25199257520408436),
 (&#39;tv&#39;, 0.2272489906622773),
 (&#39;shows&#39;, 0.17306120187797722),
 (&#39;network&#39;, 0.0945769606612006),
 (&#39;theme&#39;, 0.08517609400440743),
 (&#39;company&#39;, 0.08181918094364625),
 (&#39;producer&#39;, 0.07445065395128529),
 (&#39;executive&#39;, 0.05932842541421376),
 (&#39;broadcast&#39;, 0.057600770775431076),
 (&#39;series&#39;, 0.0476476709374204)]</code></pre><pre><code class="python">from sklearn.metrics.pairwise import cosine_similarity
for i in range(20):
    # Calculate cosine similarity
    similarities = cosine_similarity(tf_idf.T)
    np.fill_diagonal(similarities, 0)

    # Extract label to merge into and from where
    topic_sizes = docs_df.groupby([&#39;Topic&#39;]).count().sort_values(&quot;Doc&quot;, ascending=False).reset_index()
    topic_to_merge = topic_sizes.iloc[-1].Topic
    topic_to_merge_into = np.argmax(similarities[topic_to_merge + 1]) - 1

    # Adjust topics
    docs_df.loc[docs_df.Topic == topic_to_merge, &quot;Topic&quot;] = topic_to_merge_into
    old_topics = docs_df.sort_values(&quot;Topic&quot;).Topic.unique()
    map_topics = {old_topic: index - 1 for index, old_topic in enumerate(old_topics)}
    docs_df.Topic = docs_df.Topic.map(map_topics)
    docs_per_topic = docs_df.groupby([&#39;Topic&#39;], as_index = False).agg({&#39;Doc&#39;: &#39; &#39;.join})

    # Calculate new topic words
    m = len(data)
    tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m)
    top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)

topic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(10)</code></pre><pre><code class="python">top_n_words[51][:10]</code></pre><pre><code>[(&#39;trump&#39;, 0.10467902584887456),
 (&#39;president&#39;, 0.06072602666892069),
 (&#39;2020&#39;, 0.03411193922678639),
 (&#39;america&#39;, 0.03269916901768407),
 (&#39;democratic&#39;, 0.032277888078611434),
 (&#39;donald&#39;, 0.029263497562009327),
 (&#39;democrats&#39;, 0.0268655653148694),
 (&#39;election&#39;, 0.02609372385412432),
 (&#39;presidential&#39;, 0.025912575696792114),
 (&#39;bernie&#39;, 0.025237479236590536)]</code></pre><pre><code class="python">top_n_words[50][:10]</code></pre><pre><code>[(&#39;don&#39;, 0.03970939022232294),
 (&#39;people&#39;, 0.03339189616992504),
 (&#39;anxiety&#39;, 0.03049151218674598),
 (&#39;life&#39;, 0.023705264451986674),
 (&#39;mental&#39;, 0.023679815071311398),
 (&#39;doesn&#39;, 0.02318471421412793),
 (&#39;disorder&#39;, 0.02080708641397244),
 (&#39;need&#39;, 0.01934262579411308),
 (&#39;like&#39;, 0.01924398264657584),
 (&#39;just&#39;, 0.019145351423775627)]</code></pre></section>
  </body>
</html>
<script>
const ws_url = "ws://" + location.host + "/serum_live_reloader";
var ws;

connect();

function connect() {
  ws = new WebSocket(ws_url);
  ws.onmessage = onMessage;
  ws.onclose = onClose;
}

function onMessage(e) {
  if (e.data === "reload") {
    location.reload();
  }
}

function onClose(e) {
  console.warn("WebSocket disconnected from server. Reconnecting in 10 seconds.");
  setTimeout(connect, 10000)
}
</script>
