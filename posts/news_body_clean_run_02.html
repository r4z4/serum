
<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Newsgroup 20 Dataset with Body - Clean Run 02 - r4z4 Site</title>
    <link rel="stylesheet" href="/serum/assets/css/style.css">
    <link rel="stylesheet" href="/serum/assets/css/home_animation.css">
    <link rel="stylesheet" href="/serum/assets/css/markdown.css">
  </head>
  <body>
    
<div id="navbar_div">
  <ul id="navbar">
    <li><a href="/serum/index.html">Home</a></li>
    <li><a href="/serum/posts">Posts</a></li>
  </ul>
</div>

    <h1 class="base-header"><a href="/serum/">Various Writings and Observations</a></h1>
    <section class="md-body"><h1 id="newsgroup-20-dataset-with-body---clean-run-02">Newsgroup 20 Dataset with Body - Clean Run 02</h1><p>Posted on Wednesday, 16 Aug 2023 by r4z4</p><p>Tags:</p><ul><li><a href="/serum/tags/NLP">NLP</a></li><li><a href="/serum/tags/news">news</a></li></ul><p>We’ll keep the same pattern here and continnue with some simple data augmentation. Since this is run #3, that means we’ll do the simple insertion.</p><hr class="thin"/><pre><code class="python">import numpy as np
import regex as re
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import keras
import nltk
import os
import sys
sys.path.insert(1, os.path.join(sys.path[0], &#39;..&#39;))
import utils

import tensorflow as tf
from sklearn.preprocessing import LabelEncoder
from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences</code></pre><pre><code class="python"># Load model
model_file = &#39;models/newsgroup_body_clean_model&#39;
model = keras.models.load_model(model_file)</code></pre><h2 id="we-will-augment-out-data-now.-run-02-=-synonym-replacement">We will augment out data now. Run 02 = Synonym Replacement</h2><pre><code class="python">df = pd.read_pickle(&#39;../data/dataframes/newsgroup_body_cleaned_exploded.pkl&#39;)</code></pre><pre><code class="python">from IPython.display import display, HTML
display(HTML(&quot;&lt;style&gt;.container { width:100% !important; }&lt;/style&gt;&quot;))
# Replace Rejoin
df[&#39;exploded_body&#39;] = df[&#39;exploded_body&#39;].apply(lambda x: utils.replace_rejoin(x))
print(df.sample(frac=1).reset_index(drop=True).loc[:,[&#39;newsgroup&#39;, &#39;exploded_body&#39;]].head().to_markdown())</code></pre><table><thead><tr><th style="text-align: right"></th><th style="text-align: left">newsgroup</th><th style="text-align: left">exploded_body</th></tr></thead><tbody><tr><td style="text-align: right">0</td><td style="text-align: left">religion</td><td style="text-align: left">apr god promis chronicl fail fill asa said unto pick up ye asa judah benjamin godhead ye ye seek found ye forsak fo</td></tr><tr><td style="text-align: right">1</td><td style="text-align: left">comp_elec</td><td style="text-align: left">page setup notepad previou articl joel jachhawaiiedu joel aycock drop a line struggl margin problem age well final</td></tr><tr><td style="text-align: right">2</td><td style="text-align: left">politics</td><td style="text-align: left">convict ye survey present accord mr cramer valu call median- one use thi make us believ manly plu sex partner manly</td></tr><tr><td style="text-align: right">3</td><td style="text-align: left">sci_med</td><td style="text-align: left">plato sinc whole enterpris philosophi wa essenti defin although got hi suffice wrong definit identifi import intee</td></tr><tr><td style="text-align: right">4</td><td style="text-align: left">comp_elec</td><td style="text-align: left">dx eisa bu s unused speed local bu — ani idea andrea dist institut fuer computersystem eth zuerich electronic ma</td></tr></tbody></table><pre><code class="python">all_categories = [&#39;sport&#39;, &#39;autos&#39;, &#39;religion&#39;, &#39;comp_elec&#39;, &#39;sci_med&#39;, &#39;seller&#39;, &#39;politics&#39;]
# We&#39;ll use all
target_categories = [&#39;sport&#39;, &#39;autos&#39;, &#39;religion&#39;, &#39;comp_elec&#39;, &#39;sci_med&#39;, &#39;seller&#39;, &#39;politics&#39;]</code></pre><pre><code class="python"># container for sentences
X = np.array([s for s in df[&#39;exploded_body&#39;]])
# container for sentences
y = np.array([n for n in df[&#39;newsgroup&#39;]])</code></pre><pre><code class="python">from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
y = encoder.fit_transform(df[&#39;newsgroup&#39;])</code></pre><pre><code class="python">from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    stratify=y, 
                                                    test_size=0.25)

classes = np.unique(y_train)
mapping = dict(zip(classes, target_categories))

len(X_train), len(X_test), classes, mapping</code></pre><pre><code>(51882,
 17295,
 array([0, 1, 2, 3, 4, 5, 6]),
 {0: &#39;sport&#39;,
  1: &#39;autos&#39;,
  2: &#39;religion&#39;,
  3: &#39;comp_elec&#39;,
  4: &#39;sci_med&#39;,
  5: &#39;seller&#39;,
  6: &#39;politics&#39;})</code></pre><pre><code class="python"># model parameters
vocab_size = 1200
embedding_dim = 16
max_length = 120
trunc_type=&#39;post&#39;
padding_type=&#39;post&#39;
oov_tok = &quot;&lt;OOV&gt;&quot;</code></pre><pre><code class="python"># tokenize sentences
tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(X_train)
word_index = tokenizer.word_index

# convert train dataset to sequence and pad sequences
train_sequences = tokenizer.texts_to_sequences(X_train)
train_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)

# convert validation dataset to sequence and pad sequences
validation_sequences = tokenizer.texts_to_sequences(X_test)
validation_padded = pad_sequences(validation_sequences, padding=padding_type, maxlen=max_length)</code></pre><pre><code class="python"># fit model
num_epochs = 20
history = model.fit(train_padded, y_train, 
                    epochs=num_epochs, verbose=1,
                    validation_split=0.3)

# predict values
pred = model.predict(validation_padded)</code></pre><pre><code>2023-06-21 00:29:07.052575: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 17432160 exceeds 10% of free system memory.


Epoch 1/20
1135/1135 [==============================] - 5s 3ms/step - loss: 2.1807 - accuracy: 0.4020 - val_loss: 1.3669 - val_accuracy: 0.5232
Epoch 2/20
1135/1135 [==============================] - 4s 3ms/step - loss: 1.1810 - accuracy: 0.5877 - val_loss: 0.9958 - val_accuracy: 0.6583
Epoch 3/20
1135/1135 [==============================] - 5s 4ms/step - loss: 0.8793 - accuracy: 0.6990 - val_loss: 0.7855 - val_accuracy: 0.7329
Epoch 4/20
1135/1135 [==============================] - 4s 4ms/step - loss: 0.7055 - accuracy: 0.7607 - val_loss: 0.6652 - val_accuracy: 0.7784
Epoch 5/20
1135/1135 [==============================] - 4s 3ms/step - loss: 0.6038 - accuracy: 0.7972 - val_loss: 0.6038 - val_accuracy: 0.7987
Epoch 6/20
1135/1135 [==============================] - 4s 3ms/step - loss: 0.5404 - accuracy: 0.8177 - val_loss: 0.5597 - val_accuracy: 0.8115
Epoch 7/20
1135/1135 [==============================] - 4s 4ms/step - loss: 0.4974 - accuracy: 0.8321 - val_loss: 0.5313 - val_accuracy: 0.8202
Epoch 8/20
1135/1135 [==============================] - 4s 4ms/step - loss: 0.4664 - accuracy: 0.8427 - val_loss: 0.5147 - val_accuracy: 0.8280
Epoch 9/20
1135/1135 [==============================] - 4s 4ms/step - loss: 0.4438 - accuracy: 0.8496 - val_loss: 0.5014 - val_accuracy: 0.8297
Epoch 10/20
1135/1135 [==============================] - 4s 3ms/step - loss: 0.4244 - accuracy: 0.8560 - val_loss: 0.4904 - val_accuracy: 0.8353
Epoch 11/20
1135/1135 [==============================] - 4s 4ms/step - loss: 0.4088 - accuracy: 0.8627 - val_loss: 0.4824 - val_accuracy: 0.8359
Epoch 12/20
1135/1135 [==============================] - 4s 4ms/step - loss: 0.3961 - accuracy: 0.8648 - val_loss: 0.4782 - val_accuracy: 0.8382
Epoch 13/20
1135/1135 [==============================] - 4s 4ms/step - loss: 0.3851 - accuracy: 0.8675 - val_loss: 0.4762 - val_accuracy: 0.8373
Epoch 14/20
1135/1135 [==============================] - 4s 3ms/step - loss: 0.3761 - accuracy: 0.8711 - val_loss: 0.4723 - val_accuracy: 0.8421
Epoch 15/20
1135/1135 [==============================] - 4s 3ms/step - loss: 0.3679 - accuracy: 0.8736 - val_loss: 0.4753 - val_accuracy: 0.8421
Epoch 16/20
1135/1135 [==============================] - 4s 3ms/step - loss: 0.3606 - accuracy: 0.8758 - val_loss: 0.4741 - val_accuracy: 0.8422
Epoch 17/20
1135/1135 [==============================] - 4s 3ms/step - loss: 0.3546 - accuracy: 0.8789 - val_loss: 0.4749 - val_accuracy: 0.8427
Epoch 18/20
1135/1135 [==============================] - 4s 4ms/step - loss: 0.3496 - accuracy: 0.8804 - val_loss: 0.4749 - val_accuracy: 0.8424
Epoch 19/20
1135/1135 [==============================] - 4s 4ms/step - loss: 0.3430 - accuracy: 0.8823 - val_loss: 0.4765 - val_accuracy: 0.8430
Epoch 20/20
1135/1135 [==============================] - 4s 3ms/step - loss: 0.3397 - accuracy: 0.8823 - val_loss: 0.4821 - val_accuracy: 0.8353
541/541 [==============================] - 1s 1ms/step</code></pre><pre><code class="python">import os

file_name = &#39;run_02&#39;
plot_type = &#39;history&#39;
model_name = &#39;newsgroup_body_clean&#39;
#####
os.makedirs(f&quot;images/{plot_type}&quot;, exist_ok=True)
os.makedirs(f&quot;images/{plot_type}/{model_name}&quot;, exist_ok=True)
save_path = f&#39;images/{plot_type}/{model_name}/{file_name}.png&#39; 

utils.plot_history_and_save(history, save_path)</code></pre><p><img src="/assets/images/news/body_clean_run_02.png" alt="png"/></p><hr class="thin"/><pre><code class="python"># reviews on which we need to predict
sentence = [&quot;son of genuine vinyl records 4sale&quot;]

# convert to a sequence
sequences = tokenizer.texts_to_sequences(sentence)

# pad the sequence
padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)

# preict the label
print(model.predict(padded))</code></pre><pre><code>1/1 [==============================] - 0s 19ms/step
[[0.36069816 0.63863254 0.38042638 0.34472513 0.38515127 0.27881363
  0.4187051 ]]</code></pre><pre><code class="python"># TensorFlow SavedModel format =&gt; .keras
model_file = &#39;models/newsgroup_body_clean_model&#39;
model.save(model_file)</code></pre></section>
  </body>
</html>
