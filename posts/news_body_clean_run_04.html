
<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Newsgroup 20 Dataset with Body - Clean Run 04 - r4z4 Site</title>
    <link rel="stylesheet" href="/serum/assets/css/style.css">
    <link rel="stylesheet" href="/serum/assets/css/home_animation.css">
    <link rel="stylesheet" href="/serum/assets/css/markdown.css">
  </head>
  <body>
    
<div id="navbar_div">
  <ul id="navbar">
    <li><a href="/serum/index.html">Home</a></li>
    <li><a href="/serum/posts">Posts</a></li>
  </ul>
</div>

    <h1 class="base-header"><a href="/serum/">Various Writings and Observations</a></h1>
    <section class="md-body"><h1 id="newsgroup-20-dataset-with-body---clean-run-04">Newsgroup 20 Dataset with Body - Clean Run 04</h1><p>Posted on Wednesday, 16 Aug 2023 by r4z4</p><p>Tags:</p><ul><li><a href="/serum/tags/NLP">NLP</a></li><li><a href="/serum/tags/news">news</a></li></ul><p>After seeing some pretty lackluster results from the subject only dataset, lets now go ahead and use the body for the text that we will train on. This should offer us a lot more data, but it was a struggle wrangling it all into the dataframes. I chose to go ahead and start from the text files themselves, and I will incode some snippets of the transformations below.</p><hr class="thin"/><pre><code class="python">import numpy as np
import json
import regex as re
import pandas as pd
from tqdm.notebook import tqdm
import matplotlib.pyplot as plt
import os
import sys
sys.path.insert(1, os.path.join(sys.path[0], &#39;..&#39;))
import utils
import seaborn as sns
import keras
import nltk
import random

from sklearn.feature_extraction.text import CountVectorizer
import tensorflow as tf
from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences, to_categorical
from sklearn.naive_bayes import MultinomialNB</code></pre><pre><code class="python"># Load model
model_file = &#39;models/newsgroup_body_clean_model&#39;
model = keras.models.load_model(model_file)</code></pre><h2 id="augmentation.-run-04-=-word-swap">Augmentation. Run 04 = Word Swap</h2><pre><code class="python">df = pd.read_pickle(&#39;../data/dataframes/newsgroup_body_cleaned_exploded.pkl&#39;)</code></pre><pre><code class="python">nan_rows = df[df[&#39;exploded_body&#39;].isnull()]
print(nan_rows.to_markdown())</code></pre><table><thead><tr><th style="text-align: left">newsgroup</th><th style="text-align: left">body</th><th style="text-align: left">exploded_body</th></tr></thead><tbody></tbody></table><pre><code class="python">df = df.dropna()</code></pre><pre><code class="python">def random_swap(words, n):
    new_words = words.copy()
    for _ in range(n):
        new_words = swap_word(new_words)
    return new_words

def swap_word(new_words):
    random_idx_1 = random.randint(0, len(new_words)-1)
    random_idx_2 = random_idx_1
    counter = 0
    while random_idx_2 == random_idx_1:
        random_idx_2 = random.randint(0, len(new_words)-1)
        counter += 1
        if counter &gt; 3:
            return new_words
    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] 
    return new_words

def swap_rejoin(x):
    if len(x) &gt; 1:
        words = random_swap(x.split(), 1)
        sentence = &#39; &#39;.join(words)
        return sentence</code></pre><pre><code class="python">from IPython.display import display, HTML
display(HTML(&quot;&lt;style&gt;.container { width:100% !important; }&lt;/style&gt;&quot;))
df[&#39;exploded_body&#39;] = df[&#39;exploded_body&#39;].apply(lambda x: swap_rejoin(x))
print(df.sample(frac=1).reset_index(drop=True).loc[:,[&#39;newsgroup&#39;, &#39;exploded_body&#39;]].head().to_markdown())</code></pre><table><thead><tr><th style="text-align: right"></th><th style="text-align: left">newsgroup</th><th style="text-align: left">exploded_body</th></tr></thead><tbody><tr><td style="text-align: right">0</td><td style="text-align: left">religion</td><td style="text-align: left">search illeg weapon also arrest warrant illeg weapon found thi case no-knock warrant wa call difficult flush gun toilet atf could surround compound mark polic car could driv</td></tr><tr><td style="text-align: right">1</td><td style="text-align: left">politics</td><td style="text-align: left">took nth degre idea move peopl around abus often kill sole becaus ethnic abhorr thing especi troublesom area peopl differ ethnic group live side side long togeth think stand</td></tr><tr><td style="text-align: right">2</td><td style="text-align: left">politics</td><td style="text-align: left">armenia azerbaijan two view articl may seassmuedu pt seassmuedu paul thompson schreiber post pt armenia azerbaijan two view pt pt washington report middl east â€” pt aprilmay</td></tr><tr><td style="text-align: right">3</td><td style="text-align: left">religion</td><td style="text-align: left">latest branch davidian articl apr genevarutgersedu conditt tsdarlututexasedu paul conditt wrote think realli sad mani peopl put faith mere man even claim son later andor proph</td></tr><tr><td style="text-align: right">4</td><td style="text-align: left">politics</td><td style="text-align: left">frankli never met woman worth kill anyway ar- chrome barrel worth kill - much thi ha ruin caus recoveri near futur find martial come arm one help danger think crimin thi fault</td></tr></tbody></table><pre><code class="python">df.shape</code></pre><pre><code>(69177, 3)</code></pre><pre><code class="python">bad_indices = df[df[&#39;exploded_body&#39;].apply(lambda x: not isinstance(x, str))].index
df.drop(bad_indices, inplace = True)</code></pre><pre><code class="python">df.shape</code></pre><pre><code>(69021, 3)</code></pre><pre><code class="python">all_categories = [&#39;sport&#39;, &#39;autos&#39;, &#39;religion&#39;, &#39;comp_elec&#39;, &#39;sci_med&#39;, &#39;seller&#39;, &#39;politics&#39;]
# We&#39;ll use all
target_categories = [&#39;sport&#39;, &#39;autos&#39;, &#39;religion&#39;, &#39;comp_elec&#39;, &#39;sci_med&#39;, &#39;seller&#39;, &#39;politics&#39;]</code></pre><pre><code class="python"># container for sentences
X = np.array([s for s in df[&#39;exploded_body&#39;]])
# container for sentences
y = np.array([n for n in df[&#39;newsgroup&#39;]])</code></pre><pre><code class="python">from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
y = encoder.fit_transform(df[&#39;newsgroup&#39;])</code></pre><pre><code class="python">from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    stratify=y, 
                                                    test_size=0.25)

classes = np.unique(y_train)
mapping = dict(zip(classes, target_categories))

len(X_train), len(X_test), classes, mapping</code></pre><pre><code>(51765,
 17256,
 array([0, 1, 2, 3, 4, 5, 6]),
 {0: &#39;sport&#39;,
  1: &#39;autos&#39;,
  2: &#39;religion&#39;,
  3: &#39;comp_elec&#39;,
  4: &#39;sci_med&#39;,
  5: &#39;seller&#39;,
  6: &#39;politics&#39;})</code></pre><pre><code class="python"># model parameters
vocab_size = 1200
embedding_dim = 16
max_length = 120
trunc_type=&#39;post&#39;
padding_type=&#39;post&#39;
oov_tok = &quot;&lt;OOV&gt;&quot;</code></pre><pre><code class="python"># tokenize sentences
tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(X_train)
word_index = tokenizer.word_index

# convert train dataset to sequence and pad sequences
train_sequences = tokenizer.texts_to_sequences(X_train)
train_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)

# convert validation dataset to sequence and pad sequences
validation_sequences = tokenizer.texts_to_sequences(X_test)
validation_padded = pad_sequences(validation_sequences, padding=padding_type, maxlen=max_length)</code></pre><pre><code class="python"># fit model
num_epochs = 20
history = model.fit(train_padded, y_train, 
                    epochs=num_epochs, verbose=1,
                    validation_split=0.3)

# predict values
pred = model.predict(validation_padded)</code></pre><pre><code>Epoch 1/20
1133/1133 [==============================] - 5s 4ms/step - loss: 1.9307 - accuracy: 0.4637 - val_loss: 1.2464 - val_accuracy: 0.5558
Epoch 2/20
1133/1133 [==============================] - 4s 4ms/step - loss: 1.0477 - accuracy: 0.6299 - val_loss: 0.9275 - val_accuracy: 0.6737
Epoch 3/20
1133/1133 [==============================] - 4s 3ms/step - loss: 0.8007 - accuracy: 0.7205 - val_loss: 0.7495 - val_accuracy: 0.7422
Epoch 4/20
1133/1133 [==============================] - 4s 3ms/step - loss: 0.6566 - accuracy: 0.7731 - val_loss: 0.6498 - val_accuracy: 0.7768
Epoch 5/20
1133/1133 [==============================] - 4s 3ms/step - loss: 0.5652 - accuracy: 0.8048 - val_loss: 0.5812 - val_accuracy: 0.8042
Epoch 6/20
1133/1133 [==============================] - 4s 3ms/step - loss: 0.5019 - accuracy: 0.8275 - val_loss: 0.5361 - val_accuracy: 0.8182
Epoch 7/20
1133/1133 [==============================] - 4s 3ms/step - loss: 0.4568 - accuracy: 0.8440 - val_loss: 0.5053 - val_accuracy: 0.8303
Epoch 8/20
1133/1133 [==============================] - 4s 3ms/step - loss: 0.4222 - accuracy: 0.8570 - val_loss: 0.4783 - val_accuracy: 0.8408
Epoch 9/20
1133/1133 [==============================] - 4s 4ms/step - loss: 0.3953 - accuracy: 0.8666 - val_loss: 0.4639 - val_accuracy: 0.8476
Epoch 10/20
1133/1133 [==============================] - 4s 3ms/step - loss: 0.3747 - accuracy: 0.8737 - val_loss: 0.4501 - val_accuracy: 0.8508
Epoch 11/20
1133/1133 [==============================] - 4s 3ms/step - loss: 0.3570 - accuracy: 0.8800 - val_loss: 0.4369 - val_accuracy: 0.8554
Epoch 12/20
1133/1133 [==============================] - 4s 3ms/step - loss: 0.3431 - accuracy: 0.8835 - val_loss: 0.4397 - val_accuracy: 0.8589
Epoch 13/20
1133/1133 [==============================] - 4s 3ms/step - loss: 0.3307 - accuracy: 0.8885 - val_loss: 0.4251 - val_accuracy: 0.8625
Epoch 14/20
1133/1133 [==============================] - 4s 3ms/step - loss: 0.3200 - accuracy: 0.8925 - val_loss: 0.4242 - val_accuracy: 0.8610
Epoch 15/20
1133/1133 [==============================] - 4s 4ms/step - loss: 0.3100 - accuracy: 0.8982 - val_loss: 0.4180 - val_accuracy: 0.8654
Epoch 16/20
1133/1133 [==============================] - 4s 4ms/step - loss: 0.3023 - accuracy: 0.8988 - val_loss: 0.4145 - val_accuracy: 0.8669
Epoch 17/20
1133/1133 [==============================] - 4s 4ms/step - loss: 0.2944 - accuracy: 0.9026 - val_loss: 0.4216 - val_accuracy: 0.8618
Epoch 18/20
1133/1133 [==============================] - 4s 4ms/step - loss: 0.2889 - accuracy: 0.9040 - val_loss: 0.4137 - val_accuracy: 0.8693
Epoch 19/20
1133/1133 [==============================] - 4s 4ms/step - loss: 0.2819 - accuracy: 0.9070 - val_loss: 0.4158 - val_accuracy: 0.8685
Epoch 20/20
1133/1133 [==============================] - 4s 3ms/step - loss: 0.2765 - accuracy: 0.9082 - val_loss: 0.4122 - val_accuracy: 0.8706
540/540 [==============================] - 1s 2ms/step</code></pre><pre><code class="python">import os

file_name = &#39;run_04&#39;
plot_type = &#39;history&#39;
model_name = &#39;newsgroup_body_clean&#39;
#####
os.makedirs(f&quot;images/{plot_type}&quot;, exist_ok=True)
os.makedirs(f&quot;images/{plot_type}/{model_name}&quot;, exist_ok=True)
save_path = f&#39;images/{plot_type}/{model_name}/{file_name}.png&#39; 

utils.plot_history_and_save(history, save_path)</code></pre><p><img src="/assets/images/news/body_clean_run_04.png" alt="png"/></p><pre><code class="python"># TensorFlow SavedModel format =&gt; .keras
model_file = &#39;models/newsgroup_body_clean_model&#39;
model.save(model_file)</code></pre></section>
  </body>
</html>
