
<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Newsgroup 20 Dataset - Subject Only - Run 04 - r4z4 Site</title>
    <link rel="stylesheet" href="/assets/css/style.css">
    <link rel="stylesheet" href="/assets/css/home_animation.css">
    <link rel="stylesheet" href="/assets/css/markdown.css">
  </head>
  <body>
    
<div id="navbar_div">
  <ul id="navbar">
    <li><a href="/index.html">Home</a></li>
    <li><a href="/posts">Posts</a></li>
  </ul>
</div>

    <h1 class="base-header"><a href="/">Various Writings and Observations</a></h1>
    <section class="md-body"><h1 id="newsgroup-20-dataset---subject-only---run-04">Newsgroup 20 Dataset - Subject Only - Run 04</h1><p>Posted on Wednesday, 16 Aug 2023 by r4z4</p><p>Tags:</p><ul><li><a href="/tags/NLP">NLP</a></li><li><a href="/tags/news">news</a></li></ul><pre><code class="python">import numpy as np
import json
import regex as re
import pandas as pd
from tqdm.notebook import tqdm
import matplotlib.pyplot as plt
import utils
import seaborn as sns
import keras
import nltk
import random

from sklearn.feature_extraction.text import CountVectorizer
import tensorflow as tf
from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences, to_categorical
from sklearn.naive_bayes import MultinomialNB</code></pre><pre><code class="python"># Load model
model_file = &#39;models/newsgroup_clean_model&#39;
model = keras.models.load_model(model_file)</code></pre><h2 id="augmentation.-run-04-=-word-swap">Augmentation. Run 04 = Word Swap</h2><pre><code class="python">df = pd.read_pickle(&#39;data/dataframes/newsgroup_cleaned.pkl&#39;)</code></pre><pre><code class="python">df = df.dropna()</code></pre><pre><code class="python">def random_swap(words, n):
    new_words = words.copy()
    for _ in range(n):
        new_words = swap_word(new_words)
    return new_words

def swap_word(new_words):
    random_idx_1 = random.randint(0, len(new_words)-1)
    random_idx_2 = random_idx_1
    counter = 0
    while random_idx_2 == random_idx_1:
        random_idx_2 = random.randint(0, len(new_words)-1)
        counter += 1
        if counter &gt; 3:
            return new_words
    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] 
    return new_words

def swap_rejoin(x):
    if len(x) &gt; 1:
        words = random_swap(x.split(), 1)
        sentence = &#39; &#39;.join(words)
        return sentence</code></pre><pre><code class="python">df[&#39;subject&#39;] = df[&#39;subject&#39;].apply(lambda x: swap_rejoin(x))
print(df.sample(frac=1).reset_index(drop=True).head().to_markdown(tablefmt=&quot;grid&quot;))</code></pre><table><thead><tr><th style="text-align: left"></th><th style="text-align: left">newsgroup</th><th style="text-align: left">subject</th></tr></thead><tbody><tr><td style="text-align: left">0</td><td style="text-align: left">seller</td><td style="text-align: left">complet aix-ps2 repost manual best offer softwar</td></tr><tr><td style="text-align: left">1</td><td style="text-align: left">comp_elec</td><td style="text-align: left">summari x11r5 xon</td></tr><tr><td style="text-align: left">2</td><td style="text-align: left">sport</td><td style="text-align: left">stat al</td></tr><tr><td style="text-align: left">3</td><td style="text-align: left">comp_elec</td><td style="text-align: left">confus doe appl give us whi messag</td></tr><tr><td style="text-align: left">4</td><td style="text-align: left">comp_elec</td><td style="text-align: left">vga paradis</td></tr></tbody></table><pre><code class="python">all_categories = [&#39;sport&#39;, &#39;autos&#39;, &#39;religion&#39;, &#39;comp_elec&#39;, &#39;sci_med&#39;, &#39;seller&#39;, &#39;politics&#39;]
# We&#39;ll use all
target_categories = [&#39;sport&#39;, &#39;autos&#39;, &#39;religion&#39;, &#39;comp_elec&#39;, &#39;sci_med&#39;, &#39;seller&#39;, &#39;politics&#39;]</code></pre><pre><code class="python">df = df.dropna()</code></pre><pre><code class="python"># container for sentences
X = np.array([s for s in df[&#39;subject&#39;]])
# container for sentences
y = np.array([n for n in df[&#39;newsgroup&#39;]])</code></pre><pre><code class="python">from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
y = encoder.fit_transform(df[&#39;newsgroup&#39;])</code></pre><pre><code class="python">from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    stratify=y, 
                                                    test_size=0.25)

classes = np.unique(y_train)
mapping = dict(zip(classes, target_categories))

len(X_train), len(X_test), classes, mapping</code></pre><pre><code>(6167,
 2056,
 array([0, 1, 2, 3, 4, 5, 6]),
 {0: &#39;sport&#39;,
  1: &#39;autos&#39;,
  2: &#39;religion&#39;,
  3: &#39;comp_elec&#39;,
  4: &#39;sci_med&#39;,
  5: &#39;seller&#39;,
  6: &#39;politics&#39;})</code></pre><pre><code class="python"># model parameters
vocab_size = 1200
embedding_dim = 16
max_length = 120
trunc_type=&#39;post&#39;
padding_type=&#39;post&#39;
oov_tok = &quot;&lt;OOV&gt;&quot;</code></pre><pre><code class="python"># tokenize sentences
tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(X_train)
word_index = tokenizer.word_index

# convert train dataset to sequence and pad sequences
train_sequences = tokenizer.texts_to_sequences(X_train)
train_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)

# convert validation dataset to sequence and pad sequences
validation_sequences = tokenizer.texts_to_sequences(X_test)
validation_padded = pad_sequences(validation_sequences, padding=padding_type, maxlen=max_length)</code></pre><pre><code class="python"># fit model
num_epochs = 20
history = model.fit(train_padded, y_train, 
                    epochs=num_epochs, verbose=1,
                    validation_split=0.3)

# predict values
pred = model.predict(validation_padded)</code></pre><pre><code>Epoch 1/20
  1/135 [..............................] - ETA: 0s - loss: 0.9555 - accuracy: 0.6562135/135 [==============================] - 
                                           1s 5ms/step - loss: 1.0733 - accuracy: 0.6182 - val_loss: 1.2396 - val_accuracy: 0.5575
Epoch 2/20
135/135 [==============================] - 0s 4ms/step - loss: 1.0597 - accuracy: 0.6221 - val_loss: 1.2284 - val_accuracy: 0.5656
Epoch 3/20
135/135 [==============================] - 0s 3ms/step - loss: 1.0526 - accuracy: 0.6240 - val_loss: 1.2258 - val_accuracy: 0.5678
Epoch 4/20
135/135 [==============================] - 1s 4ms/step - loss: 1.0414 - accuracy: 0.6251 - val_loss: 1.2164 - val_accuracy: 0.5737
Epoch 5/20
135/135 [==============================] - 1s 4ms/step - loss: 1.0278 - accuracy: 0.6390 - val_loss: 1.2195 - val_accuracy: 0.5683
Epoch 6/20
135/135 [==============================] - 0s 3ms/step - loss: 1.0175 - accuracy: 0.6376 - val_loss: 1.2015 - val_accuracy: 0.5775
Epoch 7/20
135/135 [==============================] - 0s 3ms/step - loss: 1.0089 - accuracy: 0.6453 - val_loss: 1.2028 - val_accuracy: 0.5797
Epoch 8/20
135/135 [==============================] - 0s 3ms/step - loss: 0.9992 - accuracy: 0.6460 - val_loss: 1.1910 - val_accuracy: 0.5808
Epoch 9/20
135/135 [==============================] - 1s 5ms/step - loss: 0.9893 - accuracy: 0.6499 - val_loss: 1.1969 - val_accuracy: 0.5856
Epoch 10/20
135/135 [==============================] - 0s 3ms/step - loss: 0.9791 - accuracy: 0.6564 - val_loss: 1.1823 - val_accuracy: 0.5824
Epoch 11/20
135/135 [==============================] - 0s 3ms/step - loss: 0.9689 - accuracy: 0.6573 - val_loss: 1.1840 - val_accuracy: 0.5754
Epoch 12/20
135/135 [==============================] - 1s 4ms/step - loss: 0.9625 - accuracy: 0.6657 - val_loss: 1.1715 - val_accuracy: 0.5883
Epoch 13/20
135/135 [==============================] - 0s 3ms/step - loss: 0.9531 - accuracy: 0.6610 - val_loss: 1.1651 - val_accuracy: 0.5883
Epoch 14/20
135/135 [==============================] - 1s 4ms/step - loss: 0.9421 - accuracy: 0.6687 - val_loss: 1.1594 - val_accuracy: 0.5905
Epoch 15/20
135/135 [==============================] - 0s 4ms/step - loss: 0.9349 - accuracy: 0.6701 - val_loss: 1.1574 - val_accuracy: 0.5894
Epoch 16/20
135/135 [==============================] - 0s 3ms/step - loss: 0.9287 - accuracy: 0.6719 - val_loss: 1.1502 - val_accuracy: 0.5916
Epoch 17/20
135/135 [==============================] - 1s 4ms/step - loss: 0.9184 - accuracy: 0.6791 - val_loss: 1.1650 - val_accuracy: 0.5943
Epoch 18/20
135/135 [==============================] - 1s 4ms/step - loss: 0.9108 - accuracy: 0.6786 - val_loss: 1.1563 - val_accuracy: 0.5948
Epoch 19/20
135/135 [==============================] - 1s 4ms/step - loss: 0.9070 - accuracy: 0.6905 - val_loss: 1.1600 - val_accuracy: 0.5975
Epoch 20/20
135/135 [==============================] - 1s 5ms/step - loss: 0.8985 - accuracy: 0.6884 - val_loss: 1.1430 - val_accuracy: 0.6035
65/65 [==============================] - 0s 2ms/step</code></pre><pre><code class="python">import os

file_name = &#39;run_04&#39;
plot_type = &#39;history&#39;
model_name = &#39;newsgroup_clean&#39;
#####
os.makedirs(f&quot;images/{plot_type}&quot;, exist_ok=True)
os.makedirs(f&quot;images/{plot_type}/{model_name}&quot;, exist_ok=True)
save_path = f&#39;images/{plot_type}/{model_name}/{file_name}.png&#39; 

utils.plot_history_and_save(history, save_path)</code></pre><p><img src="/assets/images/news/clean_run_04.png" alt="png"/></p><hr class="thin"/><p>It does seem to be performing slightly better than our previous run, but this is still certainly nothing to write home about. Now, letâ€™s work with some real data and incorporate the body of these message into our next runs. We will save the model file incase we want to use it later in any form.</p><pre><code class="python"># TensorFlow SavedModel format =&gt; .keras
model_file = &#39;models/newsgroup_clean_model&#39;
model.save(model_file)</code></pre></section>
  </body>
</html>
