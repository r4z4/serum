
<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>TREC Dataset with EDA - Run 01 - r4z4 Site</title>
    <link rel="stylesheet" href="/serum/assets/css/style.css">
    <link rel="stylesheet" href="/serum/assets/css/home_animation.css">
    <link rel="stylesheet" href="/serum/assets/css/markdown.css">
  </head>
  <body>
    
<div id="navbar_div">
  <ul id="navbar">
    <li><a href="/serum/index.html">Home</a></li>
    <li><a href="/serum/posts">Posts</a></li>
  </ul>
</div>

    <h1 class="base-header"><a href="/serum/">Various Writings and Observations</a></h1>
    <section class="md-body"><h1 id="trec-dataset-with-eda---run-01">TREC Dataset with EDA - Run 01</h1><p>Posted on Wednesday, 16 Aug 2023 by r4z4</p><p>Tags:</p><ul><li><a href="/serum/tags/NLP">NLP</a></li><li><a href="/serum/tags/augmentation">augmentation</a></li><li><a href="/serum/tags/trec">trec</a></li></ul><p>I guess a little introduction is warranted. I have always had a need to document my progress on anything. That used to be very scattered notes on paper, then scattered notes in various note-taking software formats. Having a living place to put down my thoughts just seemed like the natural progression, and that coupled with a rejuvenation of interest in model training and NLP in general – which lends itself to plenty of ever-changing concepts and the acronyms to go along with them – led me to this. This is my journey is trying to learn this stuff.  It is here to help me, but of course maybe some other lost soul will stumble across it.</p><h3 id="the-trial:">The Trial:</h3><p>The first realization I think we all make when getting into this is the data out there is hard to come by, and from my experience that is only amplified when dealing with textual data. When you start getting into some of these large examples with extensive corpora then too, it just seems to muddy the waters. This is why I just wanted to start simple and struggle through some of these concepts again. So I chose a pretty simple dataset and just working with some basic text classification for now.</p><h3 id="the-idea:">The Idea:</h3><p>Another thing I have noticed about myself is that having a concrete idea or goal in mind always helps. In this instance that just means coming up with some meaningless, contrived example that someone probably twenty years ago might’ve asked for as a product. So for some basic text classification, the idea is that a user would come to a site, being entering their input or submit their input, and the model will then classify that text and do whatever with it, and here we will just end up suggesting some simple template. Again, this will be completely contrived and trivial and may not look the prettiest.</p><h3 id="the-dataset:">The Dataset:</h3><p>TREC dataset contains 5500 labeled questions in training set and another 500 for test set. The dataset has 6 labels, 50 level-2 labels. Average length of each sentence is 10, vocabulary size of 8700.</p><hr class="thin"/><p>Also we save it so we can reload and pickup training. Plenty of fighting with this finally settled on: seems best approach is to use the default <code class="inline">.save()</code> method for the TensorFlow SavedModel format (as opposed to a .keras or .h5 extension).</p><pre><code class="python">model_file = &#39;models/5500&#39;
model.save(model_file)</code></pre><p>This approach actually creates a directory with asset files vs. a single file. To load, simple point to dir with <code class="inline">.load()</code>.
The NumPy method <code class="inline">assert_allclose()</code> also comes in handy for some extra reassurance with the saving &amp; loading of the file.</p><pre><code class="python">loaded_model = keras.models.load_model(model_file)
np.testing.assert_allclose(
    model.predict(validation_padded), loaded_model.predict(validation_padded)
)</code></pre><hr class="thin"/><pre><code class="python"># fit model
num_epochs = 20
history = model.fit(train_padded, y_train, 
                    epochs=num_epochs, verbose=1,
                    validation_split=0.3)

# predict values
pred = model.predict(validation_padded)</code></pre><p>And these are our results from run_01:</p><pre><code class="">Epoch 1/20
271/271 [==============================] - 2s 6ms/step - loss: 1.7022 - accuracy: 0.2399 - val_loss: 1.6490 - val_accuracy: 0.2346
Epoch 2/20
271/271 [==============================] - 1s 3ms/step - loss: 1.6434 - accuracy: 0.2399 - val_loss: 1.6400 - val_accuracy: 0.2346
Epoch 3/20
271/271 [==============================] - 1s 3ms/step - loss: 1.6331 - accuracy: 0.2595 - val_loss: 1.6273 - val_accuracy: 0.2351
Epoch 4/20
271/271 [==============================] - 1s 3ms/step - loss: 1.6058 - accuracy: 0.3265 - val_loss: 1.5795 - val_accuracy: 0.3715
Epoch 5/20
271/271 [==============================] - 1s 3ms/step - loss: 1.5230 - accuracy: 0.3810 - val_loss: 1.4659 - val_accuracy: 0.3845
Epoch 6/20
271/271 [==============================] - 1s 3ms/step - loss: 1.3778 - accuracy: 0.4375 - val_loss: 1.3146 - val_accuracy: 0.4389
Epoch 7/20
271/271 [==============================] - 1s 4ms/step - loss: 1.2270 - accuracy: 0.5098 - val_loss: 1.1808 - val_accuracy: 0.5096
Epoch 8/20
271/271 [==============================] - 1s 3ms/step - loss: 1.0963 - accuracy: 0.5848 - val_loss: 1.0686 - val_accuracy: 0.6045
Epoch 9/20
271/271 [==============================] - 1s 3ms/step - loss: 0.9813 - accuracy: 0.6503 - val_loss: 0.9594 - val_accuracy: 0.6767
Epoch 10/20
271/271 [==============================] - 1s 3ms/step - loss: 0.8706 - accuracy: 0.7126 - val_loss: 0.8621 - val_accuracy: 0.7223
Epoch 11/20
271/271 [==============================] - 1s 4ms/step - loss: 0.7750 - accuracy: 0.7575 - val_loss: 0.7807 - val_accuracy: 0.7352
Epoch 12/20
271/271 [==============================] - 1s 3ms/step - loss: 0.6943 - accuracy: 0.7914 - val_loss: 0.7047 - val_accuracy: 0.7924
Epoch 13/20
...
271/271 [==============================] - 1s 4ms/step - loss: 0.3765 - accuracy: 0.8919 - val_loss: 0.4500 - val_accuracy: 0.8628
Epoch 20/20
271/271 [==============================] - 1s 3ms/step - loss: 0.3526 - accuracy: 0.8984 - val_loss: 0.4333 - val_accuracy: 0.8673
97/97 [==============================] - 0s 1ms/step</code></pre><p>Now, it is easy to be fooled by the 89% and think we are great at this. Truth is this data is very insufficient and we have not done much to really challenge our model. As we go on to subsequent runs, though, we will see this score drop and vary and then come back up, which is reassuring. It does appear to be learning, but then the real question becomes how and why it is learning certain things in certain ways, how can we change this, and more importantly what can we do about it? What tools or insights can it give us, and from that, what value can we derive from it?</p><p>I will be printing and saving some simple visulaization images for later reference. I do not want to examine them all now without much to compare them too in these early stages, but they are certain valuable artifacts worth saving so that we can
revisit them later. I will just simply create a directory to store them for later viewing.</p><pre><code class="python">print(pred)

    [[0.10461694 0.89111507 0.00184274 0.04174655 0.14603339 0.99627024]
    [0.02512768 0.07283066 0.14759105 0.98934025 0.1748144  0.80831176]
    [0.32353488 0.58661866 0.8057708  0.11234509 0.5822517  0.07421786]
    ...
    [0.2541242  0.75612414 0.49566412 0.08180005 0.30037883 0.2799908 ]
    [0.2660918  0.73733705 0.48640287 0.09529052 0.3022969  0.28448245]
    [0.40638423 0.8813672  0.9125333  0.02715247 0.2143805  0.03576801]]

print(y_test)

    [[0. 0. 0. 0. 0. 1.]
    [0. 0. 0. 1. 0. 0.]
    [0. 0. 1. 0. 0. 0.]
    ...
    [0. 0. 0. 0. 0. 1.]
    [0. 0. 0. 0. 0. 1.]
    [0. 0. 1. 0. 0. 0.]]</code></pre><hr class="thin"/><h4 id="the-dataset-and-documentation-can-be-found-here">The dataset and documentation can be found <a href="https://cogcomp.seas.upenn.edu/Data/QA/QC/">here</a></h4><h4 id="you-can-also-access-the-data-via-pytorch.-details-can-be-found-on-pytorch-docs">You can also access the data via PyTorch. Details can be found <a href="https://pytorchnlp.readthedocs.io/en/latest/source/torchnlp.datasets.html">on PyTorch docs</a></h4><pre><code class="python">    torchnlp.datasets.trec_dataset(directory=&#39;data/trec/&#39;, train=False, test=False, train_filename=&#39;train_5500.label&#39;,
    test_filename=&#39;TREC_10.label&#39;, check_files=[&#39;train_5500.label&#39;], urls=[&#39;http://cogcomp.org/Data/QA/QC/train_5500
    label&#39;, &#39;http://cogcomp.org/Data/QA/QC/TREC_10.label&#39;], fine_grained=False)</code></pre><p>Let’s see if it works with Notebook Output</p><hr class="thin"/><pre><code class="python">_________________________________________________________________
Layer (type)                Output Shape              Param #   
=================================================================
embedding (Embedding)       (None, 120, 128)          1663616   
                                                                
global_average_pooling1d (G  (None, 128)              0         
lobalAveragePooling1D)                                          
                                                                
dense (Dense)               (None, 24)                3096      
                                                                
dense_1 (Dense)             (None, 6)                 150       
                                                                
=================================================================
Total params: 1,666,862
Trainable params: 1,666,862
Non-trainable params: 0
_________________________________________________________________</code></pre><h5 id="you-can-find-the-notebook-here">You can find the notebook <a href="http://www.github.com/r4z4">here</a></h5><hr class="thin"/><p>For the sake of brevity I’ll just highlight some of the main steps, but all in all it is a pretty basic model. When we do a <code class="inline">mode.summary()</code> on it, we get:</p><pre><code class="python">_________________________________________________________________
Layer (type)                Output Shape              Param #   
=================================================================
embedding_1 (Embedding)     (None, 120, 16)           19200     
                                                                
global_average_pooling1d_1   (None, 16)               0         
(GlobalAveragePooling1D)                                        
                                                                
dense_2 (Dense)             (None, 24)                408       
                                                                
dense_3 (Dense)             (None, 6)                 150       
                                                                
=================================================================
Total params: 19,758
Trainable params: 19,758
Non-trainable params: 0
_________________________________________________________________</code></pre></section>
  </body>
</html>
