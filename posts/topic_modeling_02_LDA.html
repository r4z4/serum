
<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Topic Modeling on Surface Trivia Question Dataset | Run 2 - Latent Dirichlet Allocation - r4z4 Site</title>
    <link rel="stylesheet" href="/serum/assets/css/style.css">
    <link rel="stylesheet" href="/serum/assets/css/home_animation.css">
    <link rel="stylesheet" href="/serum/assets/css/markdown.css">
  </head>
  <body>
    
<div id="navbar_div">
  <ul id="navbar">
    <li><a href="/serum/index.html">Home</a></li>
    <li><a href="/serum/posts">Posts</a></li>
  </ul>
</div>

    <h1 class="base-header"><a href="/serum/">Various Writings and Observations</a></h1>
    <section class="md-body"><h1 id="topic-modeling-on-surface-trivia-question-dataset-|-run-2---latent-dirichlet-allocation">Topic Modeling on Surface Trivia Question Dataset | Run 2 - Latent Dirichlet Allocation</h1><p>Posted on Wednesday, 16 Aug 2023 by r4z4</p><p>Tags:</p><ul><li><a href="/serum/tags/NLP">NLP</a></li><li><a href="/serum/tags/text-classification">text-classification</a></li></ul><hr class="thin"/><p>I wanted to go back and try a more traditional approach to the problem to have a comparison point for our transformers run. LDA is a tried
and true method for topic modeling, but as we will see, it requires a lot more effort than our previous example.</p><hr class="thin"/><pre><code class="python">import pandas as pd
import numpy as np
import json
import re
import gensim</code></pre><pre><code class="python">df = pd.read_json(&quot;trivia_data.json&quot;)</code></pre><pre><code class="python">## Gonna take a while to embed all the data (&gt; 2hrs on CPU). Lets just use 20% of the data
# n = 20
# df = df.head(int(len(df)*(n/100)))

data = df[&#39;corrected_question&#39;]
data_array = np.array([e for e in df[&#39;corrected_question&#39;]])</code></pre><pre><code class="python">import gensim
from gensim.utils import simple_preprocess
import nltk
nltk.download(&#39;stopwords&#39;)
from nltk.corpus import stopwords
stop_words = stopwords.words(&#39;english&#39;)
stop_words.extend([&#39;from&#39;, &#39;subject&#39;, &#39;re&#39;, &#39;edu&#39;, &#39;use&#39;])
def sent_to_words(sentences):
    for sentence in sentences:
        # deacc=True removes punctuations
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))
def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) 
             if word not in stop_words] for doc in texts]

data_words = list(sent_to_words(data_array))
# remove stop words
data_words = remove_stopwords(data_words)
print(data_words[:1][0][:30])</code></pre><pre><code>[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!


[&#39;many&#39;, &#39;movies&#39;, &#39;stanley&#39;, &#39;kubrick&#39;, &#39;direct&#39;]</code></pre><pre><code class="python">import gensim.corpora as corpora
# Create Dictionary
id2word = corpora.Dictionary(data_words)
# Create Corpus
texts = data_words
# Term Document Frequency
corpus = [id2word.doc2bow(text) for text in texts]
# View
print(corpus[:1][0][:30])</code></pre><pre><code>[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]</code></pre><pre><code class="python">from pprint import pprint
# number of topics
num_topics = 10
# Build LDA model
lda_model = gensim.models.LdaMulticore(corpus=corpus,
                                       id2word=id2word,
                                       num_topics=num_topics)
# Print the Keyword in the 10 topics
pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]</code></pre><pre><code>[(0,
  &#39;0.017*&quot;many&quot; + 0.010*&quot;name&quot; + 0.009*&quot;whose&quot; + 0.007*&quot;list&quot; + &#39;
  &#39;0.006*&quot;different&quot; + 0.006*&quot;school&quot; + 0.006*&quot;river&quot; + 0.005*&quot;also&quot; + &#39;
  &#39;0.005*&quot;common&quot; + 0.005*&quot;awards&quot;&#39;),
 (1,
  &#39;0.018*&quot;name&quot; + 0.015*&quot;whose&quot; + 0.011*&quot;many&quot; + 0.011*&quot;played&quot; + 0.009*&quot;also&quot; &#39;
  &#39;+ 0.009*&quot;team&quot; + 0.008*&quot;founded&quot; + 0.007*&quot;city&quot; + 0.007*&quot;sports&quot; + &#39;
  &#39;0.005*&quot;place&quot;&#39;),
 (2,
  &#39;0.027*&quot;name&quot; + 0.018*&quot;many&quot; + 0.015*&quot;whose&quot; + 0.013*&quot;people&quot; + 0.010*&quot;also&quot; &#39;
  &#39;+ 0.007*&quot;one&quot; + 0.007*&quot;person&quot; + 0.007*&quot;play&quot; + 0.006*&quot;city&quot; + &#39;
  &#39;0.005*&quot;company&quot;&#39;),
 (3,
  &#39;0.017*&quot;whose&quot; + 0.013*&quot;many&quot; + 0.011*&quot;team&quot; + 0.009*&quot;people&quot; + &#39;
  &#39;0.008*&quot;football&quot; + 0.007*&quot;also&quot; + 0.007*&quot;school&quot; + 0.006*&quot;list&quot; + &#39;
  &#39;0.006*&quot;place&quot; + 0.006*&quot;american&quot;&#39;),
 (4,
  &#39;0.019*&quot;show&quot; + 0.016*&quot;television&quot; + 0.015*&quot;whose&quot; + 0.012*&quot;name&quot; + &#39;
  &#39;0.008*&quot;list&quot; + 0.006*&quot;river&quot; + 0.006*&quot;many&quot; + 0.006*&quot;theme&quot; + 0.005*&quot;work&quot; &#39;
  &#39;+ 0.005*&quot;military&quot;&#39;),
 (5,
  &#39;0.016*&quot;also&quot; + 0.015*&quot;list&quot; + 0.009*&quot;team&quot; + 0.008*&quot;whose&quot; + 0.008*&quot;many&quot; + &#39;
  &#39;0.006*&quot;city&quot; + 0.006*&quot;people&quot; + 0.006*&quot;country&quot; + 0.006*&quot;teams&quot; + &#39;
  &#39;0.005*&quot;place&quot;&#39;),
 (6,
  &#39;0.020*&quot;many&quot; + 0.016*&quot;whose&quot; + 0.013*&quot;list&quot; + 0.009*&quot;also&quot; + 0.008*&quot;count&quot; &#39;
  &#39;+ 0.008*&quot;name&quot; + 0.007*&quot;company&quot; + 0.006*&quot;river&quot; + 0.005*&quot;one&quot; + &#39;
  &#39;0.005*&quot;awards&quot;&#39;),
 (7,
  &#39;0.023*&quot;name&quot; + 0.014*&quot;whose&quot; + 0.011*&quot;also&quot; + 0.009*&quot;many&quot; + 0.009*&quot;city&quot; + &#39;
  &#39;0.009*&quot;located&quot; + 0.008*&quot;team&quot; + 0.006*&quot;river&quot; + 0.005*&quot;state&quot; + &#39;
  &#39;0.005*&quot;country&quot;&#39;),
 (8,
  &#39;0.030*&quot;whose&quot; + 0.015*&quot;company&quot; + 0.013*&quot;list&quot; + 0.011*&quot;people&quot; + &#39;
  &#39;0.009*&quot;shows&quot; + 0.009*&quot;many&quot; + 0.008*&quot;name&quot; + 0.007*&quot;music&quot; + 0.007*&quot;team&quot; &#39;
  &#39;+ 0.006*&quot;count&quot;&#39;),
 (9,
  &#39;0.023*&quot;whose&quot; + 0.016*&quot;name&quot; + 0.011*&quot;one&quot; + 0.010*&quot;list&quot; + 0.009*&quot;also&quot; + &#39;
  &#39;0.009*&quot;people&quot; + 0.009*&quot;place&quot; + 0.007*&quot;many&quot; + 0.007*&quot;office&quot; + &#39;
  &#39;0.006*&quot;work&quot;&#39;)]</code></pre><pre><code class="python">import pyLDAvis.gensim
import os
import pickle 
import pyLDAvis
# Visualize the topics
pyLDAvis.enable_notebook()
os.makedirs(&#39;./results&#39;, exist_ok=True)
LDAvis_data_filepath = os.path.join(&#39;./results/ldavis_prepared_&#39;+str(num_topics))
# # this is a bit time consuming - make the if statement True
# # if you want to execute visualization prep yourself
if 1 == 1:
    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)
    with open(LDAvis_data_filepath, &#39;wb&#39;) as f:
        pickle.dump(LDAvis_prepared, f)
# load the pre-prepared pyLDAvis data from disk
with open(LDAvis_data_filepath, &#39;rb&#39;) as f:
    LDAvis_prepared = pickle.load(f)
pyLDAvis.save_html(LDAvis_prepared, &#39;./results/ldavis_prepared_&#39;+ str(num_topics) +&#39;.html&#39;)
LDAvis_prepared</code></pre><p><img src="/assets/images/topic-modeling/02_LDA.png#img-thumbnail" alt="png"/></p></section>
  </body>
</html>
