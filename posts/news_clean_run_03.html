
<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Newsgroup 20 Dataset - Subject Only - Run 03 - r4z4 Site</title>
    <link rel="stylesheet" href="/assets/css/style.css">
    <link rel="stylesheet" href="/assets/css/home_animation.css">
    <link rel="stylesheet" href="/assets/css/markdown.css">
  </head>
  <body>
    
<div id="navbar_div">
  <ul id="navbar">
    <li><a href="/index.html">Home</a></li>
    <li><a href="/posts">Posts</a></li>
  </ul>
</div>

    <h1 class="base-header"><a href="/">Various Writings and Observations</a></h1>
    <section class="md-body"><h1 id="newsgroup-20-dataset---subject-only---run-03">Newsgroup 20 Dataset - Subject Only - Run 03</h1><p>Posted on Wednesday, 16 Aug 2023 by r4z4</p><p>Tags:</p><ul><li><a href="/tags/NLP">NLP</a></li><li><a href="/tags/news">news</a></li></ul><pre><code class="python">import numpy as np
import json
import regex as re
import pandas as pd
from tqdm.notebook import tqdm
import matplotlib.pyplot as plt
import utils
import seaborn as sns
import keras
import nltk
import random

from sklearn.feature_extraction.text import CountVectorizer
import tensorflow as tf
from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences, to_categorical
from sklearn.naive_bayes import MultinomialNB</code></pre><pre><code class="python"># Load model
model_file = &#39;models/newsgroup_clean_model&#39;
model = keras.models.load_model(model_file)</code></pre><h2 id="augmentation.-run-03-=-synonym-insertion">Augmentation. Run 03 = Synonym Insertion</h2><pre><code class="python">df = pd.read_pickle(&#39;data/dataframes/newsgroup_cleaned.pkl&#39;)</code></pre><pre><code class="python">def random_insertion(words, n):
    new_words = words.copy()
    for _ in range(n):
        add_word(new_words)
    return new_words

def add_word(new_words):
    synonyms = []
    counter = 0
    while len(synonyms) &lt; 1:
        random_word = new_words[random.randint(0, len(new_words)-1)]
        synonyms = utils.get_synonyms(random_word)
        counter += 1
        if counter &gt;= 10:
            return
    random_synonym = synonyms[0]
    random_idx = random.randint(0, len(new_words)-1)
    new_words.insert(random_idx, random_synonym)
    
def insert_rejoin(x):
    if len(x) &gt; 1:
        words = random_insertion(x.split(), 1)
        sentence = &#39; &#39;.join(words)
        return sentence</code></pre><pre><code class="python">df[&#39;subject&#39;] = df[&#39;subject&#39;].apply(lambda x: insert_rejoin(x))
print(df.sample(frac=1).reset_index(drop=True).head().to_markdown(tablefmt=&quot;grid&quot;))</code></pre><pre><code>+----+-------------+--------------------------------------------------+
|    | newsgroup   | subject                                          |
+====+=============+==================================================+
|  0 | comp_elec   | pgp system of rules idea ibm system              |
+----+-------------+--------------------------------------------------+
|  1 | comp_elec   | connect ten digitis x repost                     |
+----+-------------+--------------------------------------------------+
|  2 | autos       | execute carb cleaner - work perform carb rebuild |
+----+-------------+--------------------------------------------------+
|  3 | comp_elec   | need card x win popup menu packag                |
+----+-------------+--------------------------------------------------+
|  4 | politics    | nineteenth 19th centuri capit                    |
+----+-------------+--------------------------------------------------+</code></pre><p>These EDA methods can actually end up producing some NaN types which we need to discard.</p><pre><code class="python">nan_rows = df[df[&#39;subject&#39;].isnull()]
print(nan_rows.to_markdown(tablefmt=&quot;grid&quot;))</code></pre><pre><code>+-------+-------------+-----------+
|       | newsgroup   | subject   |
+=======+=============+===========+
| 10590 | politics    |           |
+-------+-------------+-----------+
| 13180 | sport       |           |
+-------+-------------+-----------+
| 16268 | religion    |           |
+-------+-------------+-----------+
| 26798 | comp_elec   |           |
+-------+-------------+-----------+
| 27808 | comp_elec   |           |
+-------+-------------+-----------+</code></pre><pre><code class="python">df = df.dropna()</code></pre><pre><code class="python">all_categories = [&#39;sport&#39;, &#39;autos&#39;, &#39;religion&#39;, &#39;comp_elec&#39;, &#39;sci_med&#39;, &#39;seller&#39;, &#39;politics&#39;]
# We&#39;ll use all
target_categories = [&#39;sport&#39;, &#39;autos&#39;, &#39;religion&#39;, &#39;comp_elec&#39;, &#39;sci_med&#39;, &#39;seller&#39;, &#39;politics&#39;]</code></pre><pre><code class="python"># container for sentences
X = np.array([s for s in df[&#39;subject&#39;]])
# container for sentences
y = np.array([n for n in df[&#39;newsgroup&#39;]])</code></pre><pre><code class="python">from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
y = encoder.fit_transform(df[&#39;newsgroup&#39;])</code></pre><pre><code class="python">from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    stratify=y, 
                                                    test_size=0.25)

classes = np.unique(y_train)
mapping = dict(zip(classes, target_categories))

len(X_train), len(X_test), classes, mapping</code></pre><pre><code>(6167,
 2056,
 array([0, 1, 2, 3, 4, 5, 6]),
 {0: &#39;sport&#39;,
  1: &#39;autos&#39;,
  2: &#39;religion&#39;,
  3: &#39;comp_elec&#39;,
  4: &#39;sci_med&#39;,
  5: &#39;seller&#39;,
  6: &#39;politics&#39;})</code></pre><pre><code class="python"># model parameters
vocab_size = 1200
embedding_dim = 16
max_length = 120
trunc_type=&#39;post&#39;
padding_type=&#39;post&#39;
oov_tok = &quot;&lt;OOV&gt;&quot;</code></pre><pre><code class="python"># tokenize sentences
tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(X_train)
word_index = tokenizer.word_index

# convert train dataset to sequence and pad sequences
train_sequences = tokenizer.texts_to_sequences(X_train)
train_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)

# convert validation dataset to sequence and pad sequences
validation_sequences = tokenizer.texts_to_sequences(X_test)
validation_padded = pad_sequences(validation_sequences, padding=padding_type, maxlen=max_length)</code></pre><pre><code class="python"># fit model
num_epochs = 20
history = model.fit(train_padded, y_train, 
                    epochs=num_epochs, verbose=1,
                    validation_split=0.3)

# predict values
pred = model.predict(validation_padded)</code></pre><pre><code>Epoch 1/20
135/135 [==============================] - 2s 6ms/step - loss: 1.8546 - accuracy: 0.4076 - val_loss: 1.7949 - val_accuracy: 0.4138
Epoch 2/20
135/135 [==============================] - 1s 4ms/step - loss: 1.7611 - accuracy: 0.4291 - val_loss: 1.7235 - val_accuracy: 0.4263
Epoch 3/20
135/135 [==============================] - 0s 3ms/step - loss: 1.6952 - accuracy: 0.4372 - val_loss: 1.6794 - val_accuracy: 0.4354
Epoch 4/20
135/135 [==============================] - 1s 4ms/step - loss: 1.6579 - accuracy: 0.4451 - val_loss: 1.6504 - val_accuracy: 0.4403
Epoch 5/20
135/135 [==============================] - 0s 3ms/step - loss: 1.6294 - accuracy: 0.4525 - val_loss: 1.6280 - val_accuracy: 0.4516
Epoch 6/20
135/135 [==============================] - 1s 6ms/step - loss: 1.6094 - accuracy: 0.4601 - val_loss: 1.6131 - val_accuracy: 0.4495
Epoch 7/20
135/135 [==============================] - 0s 3ms/step - loss: 1.5931 - accuracy: 0.4613 - val_loss: 1.5979 - val_accuracy: 0.4554
Epoch 8/20
135/135 [==============================] - 0s 3ms/step - loss: 1.5752 - accuracy: 0.4655 - val_loss: 1.5856 - val_accuracy: 0.4571
Epoch 9/20
135/135 [==============================] - 0s 3ms/step - loss: 1.5599 - accuracy: 0.4685 - val_loss: 1.5701 - val_accuracy: 0.4625
Epoch 10/20
135/135 [==============================] - 0s 3ms/step - loss: 1.5433 - accuracy: 0.4727 - val_loss: 1.5556 - val_accuracy: 0.4657
Epoch 11/20
135/135 [==============================] - 1s 6ms/step - loss: 1.5278 - accuracy: 0.4773 - val_loss: 1.5416 - val_accuracy: 0.4706
Epoch 12/20
135/135 [==============================] - 0s 3ms/step - loss: 1.5104 - accuracy: 0.4826 - val_loss: 1.5285 - val_accuracy: 0.4700
Epoch 13/20
135/135 [==============================] - 1s 4ms/step - loss: 1.4941 - accuracy: 0.4854 - val_loss: 1.5136 - val_accuracy: 0.4738
Epoch 14/20
135/135 [==============================] - 1s 4ms/step - loss: 1.4768 - accuracy: 0.4896 - val_loss: 1.5025 - val_accuracy: 0.4722
Epoch 15/20
135/135 [==============================] - 1s 4ms/step - loss: 1.4612 - accuracy: 0.4947 - val_loss: 1.4870 - val_accuracy: 0.4841
Epoch 16/20
135/135 [==============================] - 0s 3ms/step - loss: 1.4449 - accuracy: 0.4998 - val_loss: 1.4732 - val_accuracy: 0.4857
Epoch 17/20
135/135 [==============================] - 0s 3ms/step - loss: 1.4262 - accuracy: 0.5030 - val_loss: 1.4591 - val_accuracy: 0.4943
Epoch 18/20
135/135 [==============================] - 0s 3ms/step - loss: 1.4094 - accuracy: 0.5125 - val_loss: 1.4472 - val_accuracy: 0.4965
Epoch 19/20
135/135 [==============================] - 0s 3ms/step - loss: 1.3929 - accuracy: 0.5165 - val_loss: 1.4351 - val_accuracy: 0.4943
Epoch 20/20
135/135 [==============================] - 1s 4ms/step - loss: 1.3742 - accuracy: 0.5199 - val_loss: 1.4209 - val_accuracy: 0.4986
65/65 [==============================] - 0s 1ms/step</code></pre><pre><code class="python">import os

file_name = &#39;run_03&#39;
plot_type = &#39;history&#39;
model_name = &#39;newsgroup_clean&#39;
#####
os.makedirs(f&quot;images/{plot_type}&quot;, exist_ok=True)
os.makedirs(f&quot;images/{plot_type}/{model_name}&quot;, exist_ok=True)
save_path = f&#39;images/{plot_type}/{model_name}/{file_name}.png&#39; 

utils.plot_history_and_save(history, save_path)</code></pre><p><img src="/assets/images/news/clean_run_03.png" alt="png"/></p><pre><code class="python"># reviews on which we need to predict
sentence = [&quot;son of genuine vinyl records 4sale&quot;]

# convert to a sequence
sequences = tokenizer.texts_to_sequences(sentence)

# pad the sequence
padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)

# preict the label
print(model.predict(padded))</code></pre><pre><code>1/1 [==============================] - 0s 35ms/step
[[0.34423554 0.59764796 0.34918392 0.35519084 0.41514224 0.12569432
  0.41660988]]</code></pre><pre><code class="python"># TensorFlow SavedModel format =&gt; .keras
model_file = &#39;models/newsgroup_clean_model&#39;
model.save(model_file)</code></pre></section>
  </body>
</html>
