
<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Newsgroup 20 Dataset with Body - Clean Run 03 - r4z4 Site</title>
    <link rel="stylesheet" href="/assets/css/style.css">
    <link rel="stylesheet" href="/assets/css/home_animation.css">
    <link rel="stylesheet" href="/assets/css/markdown.css">
  </head>
  <body>
    
<div id="navbar_div">
  <ul id="navbar">
    <li><a href="/index.html">Home</a></li>
    <li><a href="/posts">Posts</a></li>
  </ul>
</div>

    <h1 class="base-header"><a href="/">Various Writings and Observations</a></h1>
    <section class="md-body"><h1 id="newsgroup-20-dataset-with-body---clean-run-03">Newsgroup 20 Dataset with Body - Clean Run 03</h1><p>Posted on Wednesday, 16 Aug 2023 by r4z4</p><p>Tags:</p><ul><li><a href="/tags/NLP">NLP</a></li><li><a href="/tags/news">news</a></li></ul><p>We’ll keep the same pattern here and continnue with some simple data augmentation. Since this is run #3, that means we’ll do the simple insertion.</p><hr class="thin"/><pre><code class="python">import numpy as np
import json
import regex as re
import pandas as pd
from tqdm.notebook import tqdm
import matplotlib.pyplot as plt
import os
import sys
sys.path.insert(1, os.path.join(sys.path[0], &#39;..&#39;))
import utils
import seaborn as sns
import keras
import nltk
import random

from sklearn.feature_extraction.text import CountVectorizer
import tensorflow as tf
from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences, to_categorical
from sklearn.naive_bayes import MultinomialNB</code></pre><pre><code class="python"># Load model
model_file = &#39;models/newsgroup_body_clean_model&#39;
model = keras.models.load_model(model_file)</code></pre><h2 id="augmentation.-run-03-=-synonym-insertion">Augmentation. Run 03 = Synonym Insertion</h2><pre><code class="python">df = pd.read_pickle(&#39;../data/dataframes/newsgroup_body_cleaned_exploded.pkl&#39;)</code></pre><pre><code class="python">def random_insertion(words, n):
    new_words = words.copy()
    for _ in range(n):
        add_word(new_words)
    return new_words

def add_word(new_words):
    synonyms = []
    counter = 0
    while len(synonyms) &lt; 1:
        random_word = new_words[random.randint(0, len(new_words)-1)]
        synonyms = utils.get_synonyms(random_word)
        counter += 1
        if counter &gt;= 10:
            return
    random_synonym = synonyms[0]
    random_idx = random.randint(0, len(new_words)-1)
    new_words.insert(random_idx, random_synonym)
    
def insert_rejoin(x):
    if len(x) &gt; 1:
        words = random_insertion(x.split(), 1)
        sentence = &#39; &#39;.join(words)
        return sentence</code></pre><pre><code class="python">from IPython.display import display, HTML
display(HTML(&quot;&lt;style&gt;.container { width:100% !important; }&lt;/style&gt;&quot;))
df[&#39;exploded_body&#39;] = df[&#39;exploded_body&#39;].apply(lambda x: insert_rejoin(x))
print(df.sample(frac=1).reset_index(drop=True).loc[:,[&#39;newsgroup&#39;, &#39;exploded_body&#39;]].head().to_markdown())</code></pre><p><style>.container { width:100% !important; }</style></p><table><thead><tr><th style="text-align: right"></th><th style="text-align: left">newsgroup</th><th style="text-align: left">exploded_body</th></tr></thead><tbody><tr><td style="text-align: right">0</td><td style="text-align: left">comp_elec</td><td style="text-align: left">fax modem best well use home dear offic bought around doe nt know data fax featur use voic mail box realli like — - captain zod zod ncubecom</td></tr><tr><td style="text-align: right">1</td><td style="text-align: left">sci_med</td><td style="text-align: left">ocd articl crnfg newshawaiiedu sharynk hawaiiedu write recent heard mental disord call obsess compuls disord caus could caus nervou breakdown ob</td></tr><tr><td style="text-align: right">2</td><td style="text-align: left">comp_elec</td><td style="text-align: left">ax max ax ax ax ax ax ax ax ax ax ax ax ax ax ax max ax ax ax ax ax ax ax ax ax ax ax ax axe ax ax max ax ax ax ax ax ax ax ax ax ax ax ax ax</td></tr><tr><td style="text-align: right">3</td><td style="text-align: left">politics</td><td style="text-align: left">peopl live differ regim unit self-percept palestinian peopl identifi palestin territori entiti ethnic forethought religi entiti incorrect palest</td></tr><tr><td style="text-align: right">4</td><td style="text-align: left">religion</td><td style="text-align: left">object valu v scientif accuraci wa year say christian moral articl srusnewsww mantiscouk mathew mathew mantiscouk wrote lpzsml unicornnottacuk</td></tr></tbody></table><p>These EDA methods can actually end up producing some NaN types which we need to discard. I think that comp_elec entry right there is also a testament to no matter how much cleaning and preprocessing you do, sometimes you just get bad data. Ideally we would drop that but I doubt it would make much of a difference here.</p><pre><code class="python">nan_rows = df[df[&#39;exploded_body&#39;].isnull()]
print(nan_rows.to_markdown())</code></pre><pre><code class="python">df = df.dropna()</code></pre><pre><code class="python">nan_rows = df[df[&#39;exploded_body&#39;].isnull()]
print(nan_rows.to_markdown())</code></pre><pre><code>| newsgroup   | body   | exploded_body   |
|-------------|--------|-----------------|</code></pre><pre><code class="python">all_categories = [&#39;sport&#39;, &#39;autos&#39;, &#39;religion&#39;, &#39;comp_elec&#39;, &#39;sci_med&#39;, &#39;seller&#39;, &#39;politics&#39;]
# We&#39;ll use all
target_categories = [&#39;sport&#39;, &#39;autos&#39;, &#39;religion&#39;, &#39;comp_elec&#39;, &#39;sci_med&#39;, &#39;seller&#39;, &#39;politics&#39;]</code></pre><pre><code class="python"># container for sentences
X = np.array([s for s in df[&#39;exploded_body&#39;]])
# container for sentences
y = np.array([n for n in df[&#39;newsgroup&#39;]])</code></pre><pre><code class="python">from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
y = encoder.fit_transform(df[&#39;newsgroup&#39;])</code></pre><pre><code class="python">from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    stratify=y, 
                                                    test_size=0.25)

classes = np.unique(y_train)
mapping = dict(zip(classes, target_categories))

len(X_train), len(X_test), classes, mapping</code></pre><pre><code>(51862,
 17288,
 array([0, 1, 2, 3, 4, 5, 6]),
 {0: &#39;sport&#39;,
  1: &#39;autos&#39;,
  2: &#39;religion&#39;,
  3: &#39;comp_elec&#39;,
  4: &#39;sci_med&#39;,
  5: &#39;seller&#39;,
  6: &#39;politics&#39;})</code></pre><pre><code class="python"># model parameters
vocab_size = 1200
embedding_dim = 16
max_length = 120
trunc_type=&#39;post&#39;
padding_type=&#39;post&#39;
oov_tok = &quot;&lt;OOV&gt;&quot;</code></pre><pre><code class="python"># tokenize sentences
tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(X_train)
word_index = tokenizer.word_index

# convert train dataset to sequence and pad sequences
train_sequences = tokenizer.texts_to_sequences(X_train)
train_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)

# convert validation dataset to sequence and pad sequences
validation_sequences = tokenizer.texts_to_sequences(X_test)
validation_padded = pad_sequences(validation_sequences, padding=padding_type, maxlen=max_length)</code></pre><pre><code class="python"># fit model
num_epochs = 20
history = model.fit(train_padded, y_train, 
                    epochs=num_epochs, verbose=1,
                    validation_split=0.3)

# predict values
pred = model.predict(validation_padded)</code></pre><pre><code>2023-06-21 00:38:23.510932: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 17425440 exceeds 10% of free system memory.


Epoch 1/20
1135/1135 [==============================] - 6s 4ms/step - loss: 1.9798 - accuracy: 0.4307 - val_loss: 1.2645 - val_accuracy: 0.5585
Epoch 2/20
1135/1135 [==============================] - 5s 4ms/step - loss: 1.0619 - accuracy: 0.6272 - val_loss: 0.9048 - val_accuracy: 0.6835
Epoch 3/20
1135/1135 [==============================] - 4s 4ms/step - loss: 0.7877 - accuracy: 0.7285 - val_loss: 0.7124 - val_accuracy: 0.7545
Epoch 4/20
1135/1135 [==============================] - 4s 3ms/step - loss: 0.6348 - accuracy: 0.7849 - val_loss: 0.6078 - val_accuracy: 0.7934
Epoch 5/20
1135/1135 [==============================] - 4s 3ms/step - loss: 0.5457 - accuracy: 0.8173 - val_loss: 0.5460 - val_accuracy: 0.8153
Epoch 6/20
1135/1135 [==============================] - 4s 3ms/step - loss: 0.4897 - accuracy: 0.8361 - val_loss: 0.5156 - val_accuracy: 0.8227
Epoch 7/20
1135/1135 [==============================] - 4s 3ms/step - loss: 0.4503 - accuracy: 0.8503 - val_loss: 0.4832 - val_accuracy: 0.8362
Epoch 8/20
1135/1135 [==============================] - 4s 4ms/step - loss: 0.4209 - accuracy: 0.8607 - val_loss: 0.4644 - val_accuracy: 0.8448
Epoch 9/20
1135/1135 [==============================] - 5s 4ms/step - loss: 0.3985 - accuracy: 0.8676 - val_loss: 0.4513 - val_accuracy: 0.8501
Epoch 10/20
1135/1135 [==============================] - 4s 3ms/step - loss: 0.3810 - accuracy: 0.8733 - val_loss: 0.4402 - val_accuracy: 0.8520
Epoch 11/20
1135/1135 [==============================] - 4s 3ms/step - loss: 0.3655 - accuracy: 0.8782 - val_loss: 0.4332 - val_accuracy: 0.8556
Epoch 12/20
1135/1135 [==============================] - 4s 4ms/step - loss: 0.3534 - accuracy: 0.8817 - val_loss: 0.4306 - val_accuracy: 0.8589
Epoch 13/20
1135/1135 [==============================] - 4s 3ms/step - loss: 0.3431 - accuracy: 0.8854 - val_loss: 0.4314 - val_accuracy: 0.8551
Epoch 14/20
1135/1135 [==============================] - 4s 3ms/step - loss: 0.3327 - accuracy: 0.8903 - val_loss: 0.4239 - val_accuracy: 0.8610
Epoch 15/20
1135/1135 [==============================] - 4s 4ms/step - loss: 0.3239 - accuracy: 0.8918 - val_loss: 0.4178 - val_accuracy: 0.8616
Epoch 16/20
1135/1135 [==============================] - 4s 3ms/step - loss: 0.3160 - accuracy: 0.8949 - val_loss: 0.4153 - val_accuracy: 0.8624
Epoch 17/20
1135/1135 [==============================] - 4s 4ms/step - loss: 0.3094 - accuracy: 0.8966 - val_loss: 0.4175 - val_accuracy: 0.8625
Epoch 18/20
1135/1135 [==============================] - 5s 4ms/step - loss: 0.3037 - accuracy: 0.8985 - val_loss: 0.4109 - val_accuracy: 0.8641
Epoch 19/20
1135/1135 [==============================] - 5s 4ms/step - loss: 0.2971 - accuracy: 0.9011 - val_loss: 0.4110 - val_accuracy: 0.8650
Epoch 20/20
1135/1135 [==============================] - 4s 3ms/step - loss: 0.2926 - accuracy: 0.9020 - val_loss: 0.4186 - val_accuracy: 0.8630
541/541 [==============================] - 1s 2ms/step</code></pre><pre><code class="python">import os

file_name = &#39;run_03&#39;
plot_type = &#39;history&#39;
model_name = &#39;newsgroup_body_clean&#39;
#####
os.makedirs(f&quot;images/{plot_type}&quot;, exist_ok=True)
os.makedirs(f&quot;images/{plot_type}/{model_name}&quot;, exist_ok=True)
save_path = f&#39;images/{plot_type}/{model_name}/{file_name}.png&#39; 

utils.plot_history_and_save(history, save_path)</code></pre><p><img src="/assets/images/news/body_clean_run_03.png" alt="png"/></p><pre><code class="python"># TensorFlow SavedModel format =&gt; .keras
model_file = &#39;models/newsgroup_body_clean_model&#39;
model.save(model_file)</code></pre></section>
  </body>
</html>
<script>
const ws_url = "ws://" + location.host + "/serum_live_reloader";
var ws;

connect();

function connect() {
  ws = new WebSocket(ws_url);
  ws.onmessage = onMessage;
  ws.onclose = onClose;
}

function onMessage(e) {
  if (e.data === "reload") {
    location.reload();
  }
}

function onClose(e) {
  console.warn("WebSocket disconnected from server. Reconnecting in 10 seconds.");
  setTimeout(connect, 10000)
}
</script>
