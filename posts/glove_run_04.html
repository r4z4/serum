
<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>GloVe Run 04 - r4z4 Site</title>
    <link rel="stylesheet" href="/assets/css/style.css">
    <link rel="stylesheet" href="/assets/css/home_animation.css">
    <link rel="stylesheet" href="/assets/css/markdown.css">
  </head>
  <body>
    
<div id="navbar_div">
  <ul id="navbar">
    <li><a href="/index.html">Home</a></li>
    <li><a href="/posts">Posts</a></li>
  </ul>
</div>

    <h1 class="base-header"><a href="/">Various Writings and Observations</a></h1>
    <section class="md-body"><h1 id="glove-run-04">GloVe Run 04</h1><p>Posted on Wednesday, 16 Aug 2023 by r4z4</p><p>Tags:</p><ul><li><a href="/tags/GloVe">GloVe</a></li><li><a href="/tags/NLP">NLP</a></li><li><a href="/tags/embeddings">embeddings</a></li></ul><p>This will be the final run here with the GloVe embeddings and the last stage in the augmentation. So far we have not really seen to much improvement and I anticipate that will be the case here, but nevertheless letâ€™s go ahead and complete the cycle.</p><hr class="thin"/><pre><code class="python">import numpy as np
import regex as re
import pandas as pd
from tqdm.notebook import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
import utils

import keras
import tensorflow as tf
from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences</code></pre><pre><code class="python">df = pd.read_pickle(&#39;./data/dataframes/outer_merged_normalized_deduped.pkl&#39;)
df.head()</code></pre><hr class="thin"/><pre><code class="python">all_categories = [&#39;sport&#39;, &#39;autos&#39;, &#39;religion&#39;, &#39;comp_elec&#39;, &#39;sci_med&#39;, &#39;seller&#39;, &#39;politics&#39;]
# We&#39;ll use all
target_categories = [&#39;sport&#39;, &#39;autos&#39;, &#39;religion&#39;, &#39;comp_elec&#39;, &#39;sci_med&#39;, &#39;seller&#39;, &#39;politics&#39;]</code></pre><h3 id="augment-our-data">Augment our data</h3><pre><code class="python"># But still need to fit the tokenizer on our original text to keep same vocak
max_tokens = 50 
X = np.array([subject for subject in df[&#39;subject&#39;]])
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)</code></pre><pre><code class="python">df[&#39;subject&#39;] = df[&#39;subject&#39;].apply(lambda x: utils.insert_rejoin(x))</code></pre><pre><code class="python"># container for sentences
X = np.array([subject for subject in df[&#39;subject&#39;]])
# container for sentences
y = np.array([subject for subject in df[&#39;newsgroup&#39;]])</code></pre><pre><code class="python">encoder = preprocessing.LabelEncoder()
y = encoder.fit_transform(df[&#39;newsgroup&#39;])</code></pre><pre><code class="python">X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    stratify=y, 
                                                    test_size=0.25)

classes = np.unique(y_train)
mapping = dict(zip(classes, target_categories))

len(X_train), len(X_test), classes, mapping</code></pre><pre><code>(6359,
 2120,
 array([0, 1, 2, 3, 4, 5, 6]),
 {0: &#39;sport&#39;,
  1: &#39;autos&#39;,
  2: &#39;religion&#39;,
  3: &#39;comp_elec&#39;,
  4: &#39;sci_med&#39;,
  5: &#39;seller&#39;,
  6: &#39;politics&#39;})</code></pre><pre><code class="python">## Vectorizing data to keep 50 words per sample.
X_train_vect = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=max_tokens, padding=&quot;post&quot;, truncating=&quot;post&quot;, value=0.)
X_test_vect  = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_tokens, padding=&quot;post&quot;, truncating=&quot;post&quot;, value=0.)

print(X_train_vect[:3])

X_train_vect.shape, X_test_vect.shape</code></pre><pre><code>[[ 921  808   10 1301 1268    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0]
 [6123   19 1241    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0]
 [  26 8243 8244  239    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0]]



((6359, 50), (2120, 50))</code></pre><pre><code class="python"># Should match previous runs
print(&quot;Vocab Size : {}&quot;.format(len(tokenizer.word_index)))</code></pre><pre><code>Vocab Size : 9734</code></pre><pre><code class="python">path = &#39;./glove.6B.50d.txt&#39;
glove_embeddings = {}
with open(path) as f:
    for line in f:
        try:
            line = line.split()
            glove_embeddings[line[0]] = np.array(line[1:], dtype=np.float32)
        except:
            continue</code></pre><pre><code class="python">embed_len = 50

word_embeddings = np.zeros((len(tokenizer.index_word)+1, embed_len))

for idx, word in tokenizer.index_word.items():
    word_embeddings[idx] = glove_embeddings.get(word, np.zeros(embed_len))</code></pre><pre><code class="python">word_embeddings[1][:10]</code></pre><pre><code>array([ 0.15272   ,  0.36181   , -0.22168   ,  0.066051  ,  0.13029   ,
        0.37075001, -0.75874001, -0.44722   ,  0.22563   ,  0.10208   ])</code></pre><hr class="thin"/><h3 id="approach-1:-glove-embeddings-flattened-(max-tokens=50,-embedding-length=300)">Approach 1: GloVe Embeddings Flattened (Max Tokens=50, Embedding Length=300)</h3><h3 id="load-previously-trained-model">Load previously trained model</h3><hr class="thin"/><pre><code class="python"># Load model
model_file = &#39;models/sparse_cat_entire&#39;
model = keras.models.load_model(model_file)
model.summary()</code></pre><pre><code>Model: &quot;SparseCategoricalEntireData&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding (Embedding)       (None, 50, 50)            486750    

 flatten (Flatten)           (None, 2500)              0         

 dense (Dense)               (None, 128)               320128    

 dense_1 (Dense)             (None, 64)                8256      

 dense_2 (Dense)             (None, 7)                 455       

=================================================================
Total params: 815,589
Trainable params: 328,839
Non-trainable params: 486,750
_________________________________________________________________</code></pre><pre><code class="python">model.weights[0][1][:10], word_embeddings[1][:10]</code></pre><pre><code>(&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=
 array([ 0.15272 ,  0.36181 , -0.22168 ,  0.066051,  0.13029 ,  0.37075 ,
        -0.75874 , -0.44722 ,  0.22563 ,  0.10208 ], dtype=float32)&gt;,
 array([ 0.15272   ,  0.36181   , -0.22168   ,  0.066051  ,  0.13029   ,
         0.37075001, -0.75874001, -0.44722   ,  0.22563   ,  0.10208   ]))</code></pre><pre><code class="python">model.fit(X_train_vect, y_train, batch_size=32, epochs=8, validation_data=(X_test_vect, y_test))</code></pre><pre><code>Epoch 1/8
199/199 [==============================] - 2s 7ms/step - loss: 0.8248 - accuracy: 0.7789 - val_loss: 0.6934 - val_accuracy: 0.7896
Epoch 2/8
199/199 [==============================] - 1s 6ms/step - loss: 0.4073 - accuracy: 0.8800 - val_loss: 0.7078 - val_accuracy: 0.7953
Epoch 3/8
199/199 [==============================] - 1s 6ms/step - loss: 0.2645 - accuracy: 0.9291 - val_loss: 0.7498 - val_accuracy: 0.7929
Epoch 4/8
199/199 [==============================] - 1s 6ms/step - loss: 0.1771 - accuracy: 0.9530 - val_loss: 0.8251 - val_accuracy: 0.7816
Epoch 5/8
199/199 [==============================] - 2s 10ms/step - loss: 0.1199 - accuracy: 0.9734 - val_loss: 0.8772 - val_accuracy: 0.7788
Epoch 6/8
199/199 [==============================] - 2s 11ms/step - loss: 0.0815 - accuracy: 0.9847 - val_loss: 0.9525 - val_accuracy: 0.7722
Epoch 7/8
199/199 [==============================] - 2s 9ms/step - loss: 0.0632 - accuracy: 0.9888 - val_loss: 1.0143 - val_accuracy: 0.7764
Epoch 8/8
199/199 [==============================] - 1s 6ms/step - loss: 0.0497 - accuracy: 0.9904 - val_loss: 1.0631 - val_accuracy: 0.7726


&lt;keras.callbacks.History at 0x7f3cbd595130&gt;</code></pre><pre><code class="python">from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

y_preds = model.predict(X_test_vect).argmax(axis=-1)

print(&quot;Test Accuracy : {}&quot;.format(accuracy_score(y_test, y_preds)))
print(&quot;\nClassification Report : &quot;)
print(classification_report(y_test, y_preds, target_names=target_categories))
print(&quot;\nConfusion Matrix : &quot;)
print(confusion_matrix(y_test, y_preds))</code></pre><pre><code>67/67 [==============================] - 1s 7ms/step
Test Accuracy : 0.7726415094339623

Classification Report : 
              precision    recall  f1-score   support

       sport       0.65      0.59      0.62       189
       autos       0.83      0.88      0.85       901
    religion       0.73      0.65      0.69       220
   comp_elec       0.70      0.77      0.73       179
     sci_med       0.73      0.68      0.70       192
      seller       0.72      0.71      0.71       221
    politics       0.81      0.76      0.78       218

    accuracy                           0.77      2120
   macro avg       0.74      0.72      0.73      2120
weighted avg       0.77      0.77      0.77      2120


Confusion Matrix : 
[[112  35  11   6   7  14   4]
 [ 19 794   6  15  24  26  17]
 [ 11  21 143  24   7   3  11]
 [  3  13  15 138   3   4   3]
 [  8  33   9   8 130   2   2]
 [ 13  44   2   2   3 156   1]
 [  6  18   9   4   4  12 165]]</code></pre><pre><code class="python"># TensorFlow SavedModel format =&gt; .keras
model_file = &#39;models/sparse_cat_entire&#39;
model.save(model_file)</code></pre><pre><code class="python">!pip install scikit-plot
from sklearn.metrics import confusion_matrix
import scikitplot as skplt
import matplotlib.pyplot as plt
import numpy as np

skplt.metrics.plot_confusion_matrix([target_categories[i] for i in y_test], [target_categories[i] for i in y_preds],
                                    normalize=True,
                                    title=&quot;Confusion Matrix&quot;,
                                    cmap=&quot;Greens&quot;,
                                    hide_zeros=True,
                                    figsize=(5,5)
                                    );
plt.xticks(rotation=90);</code></pre><hr class="thin"/><p><img src="/assets/images/glove/run_04.png#img-thumbnail" alt="png"/></p><h3 id="custom-test">Custom Test</h3><hr class="thin"/><pre><code class="python"># define documents
docs = [&#39;Democrats the Reuplicans are both the worst!&#39;,
&#39;Coyotes win 10-0&#39;,
&#39;Houston Astros defeat the Cubs&#39;,
&#39;Apple and Microsoft both make great computers&#39;,
&#39;New washer 4sale. $200&#39;]

doc_array = np.array(docs)

doc_array_vect  = pad_sequences(tokenizer.texts_to_sequences(doc_array), maxlen=max_tokens, padding=&quot;post&quot;, truncating=&quot;post&quot;, value=0.)

cstm_test_preds = model.predict(doc_array_vect).argmax(axis=-1)</code></pre><pre><code>1/1 [==============================] - 0s 179ms/step</code></pre><pre><code class="python">print(doc_array)</code></pre><pre><code>[&#39;Democrats the Reuplicans are both the worst!&#39; &#39;Coyotes win 10-0&#39;
 &#39;Houston Astros defeat the Cubs&#39;
 &#39;Apple and Microsoft both make great computers&#39; &#39;New washer 4sale. $200&#39;]</code></pre><pre><code class="python">print(doc_array_vect)</code></pre><pre><code>[[   2   53 3716    2 1299    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0]
 [  69  199   47    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0]
 [2931 4257    2 1113    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0]
 [ 124    3  461 3716  277  580 1149    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0]
 [  27  842  542    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0]]</code></pre><pre><code class="python">print(cstm_test_preds)</code></pre><pre><code>[2 1 6 1 1]</code></pre></section>
  </body>
</html>
