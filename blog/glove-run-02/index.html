<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <link rel="stylesheet" href="https://r4z4.github.io/serum/global.css">
  <link rel="stylesheet" href="https://r4z4.github.io/serum/markdown.css">
  <title>MyBlog</title>
</head>

<body>
  <section class="section">
    <div class="container">
      
<h1 class="title">
  GloVe Run 02
</h1>
<p class="subtitle"><strong>2023-08-16</strong></p>
<p>Now we will start to augment our data in order to get a better understand of what kinds of parameters matter and how they all interact. The benefit of keeping things relatively simple will allow us to see a lot more of the details that may be hidden away in some of the more complex examples that you may come across.</p>
<hr />
<pre data-lang="python" class="language-python "><code class="language-python" data-lang="python">import numpy as np
import regex as re
import pandas as pd
from tqdm.notebook import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import preprocessing
import utils

import keras
import tensorflow as tf
from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences
</code></pre>
<pre data-lang="python" class="language-python "><code class="language-python" data-lang="python">df = pd.read_pickle(&#x27;.&#x2F;data&#x2F;dataframes&#x2F;outer_merged_normalized_deduped.pkl&#x27;)
df.head()
</code></pre>
<pre data-lang="python" class="language-python "><code class="language-python" data-lang="python">all_categories = [&#x27;sport&#x27;, &#x27;autos&#x27;, &#x27;religion&#x27;, &#x27;comp_elec&#x27;, &#x27;sci_med&#x27;, &#x27;seller&#x27;, &#x27;politics&#x27;]
# We&#x27;ll use all
target_categories = [&#x27;sport&#x27;, &#x27;autos&#x27;, &#x27;religion&#x27;, &#x27;comp_elec&#x27;, &#x27;sci_med&#x27;, &#x27;seller&#x27;, &#x27;politics&#x27;]
</code></pre>
<h3 id="augment-our-data">Augment our data</h3>
<pre data-lang="python" class="language-python "><code class="language-python" data-lang="python"># But still need to fit the tokenizer on our original text to keep same vocak
max_tokens = 50 
X = np.array([subject for subject in df[&#x27;subject&#x27;]])
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)
</code></pre>
<pre data-lang="python" class="language-python "><code class="language-python" data-lang="python">df[&#x27;subject&#x27;] = df[&#x27;subject&#x27;].apply(lambda x: utils.replace_rejoin(x))
</code></pre>
<pre data-lang="python" class="language-python "><code class="language-python" data-lang="python"># container for sentences
X = np.array([subject for subject in df[&#x27;subject&#x27;]])
# container for sentences
y = np.array([subject for subject in df[&#x27;newsgroup&#x27;]])
</code></pre>
<pre data-lang="python" class="language-python "><code class="language-python" data-lang="python">encoder = preprocessing.LabelEncoder()
y = encoder.fit_transform(df[&#x27;newsgroup&#x27;])
</code></pre>
<pre data-lang="python" class="language-python "><code class="language-python" data-lang="python">X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    stratify=y, 
                                                    test_size=0.25)

classes = np.unique(y_train)
mapping = dict(zip(classes, target_categories))

len(X_train), len(X_test), classes, mapping
</code></pre>
<pre><code>(28245,
 9415,
 array([0, 1, 2, 3, 4, 5, 6]),
 {0: &#x27;sport&#x27;,
  1: &#x27;autos&#x27;,
  2: &#x27;religion&#x27;,
  3: &#x27;comp_elec&#x27;,
  4: &#x27;sci_med&#x27;,
  5: &#x27;seller&#x27;,
  6: &#x27;politics&#x27;})
</code></pre>
<pre data-lang="python" class="language-python "><code class="language-python" data-lang="python">## Vectorizing data to keep 50 words per sample.
X_train_vect = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=max_tokens, padding=&quot;post&quot;, truncating=&quot;post&quot;, value=0.)
X_test_vect  = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_tokens, padding=&quot;post&quot;, truncating=&quot;post&quot;, value=0.)

print(X_train_vect[:3])

X_train_vect.shape, X_test_vect.shape
</code></pre>
<pre><code>[[7186   20 4935    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0]
 [ 933   38 2247 1972    8   84  281    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0]
 [3365  641    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0]]



((28245, 50), (9415, 50))
</code></pre>
<pre data-lang="python" class="language-python "><code class="language-python" data-lang="python">print(&quot;Vocab Size : {}&quot;.format(len(tokenizer.word_index)))
</code></pre>
<pre><code>Vocab Size : 9734
</code></pre>
<pre data-lang="python" class="language-python "><code class="language-python" data-lang="python">path = &#x27;.&#x2F;glove.6B.50d.txt&#x27;
glove_embeddings = {}
with open(path) as f:
    for line in f:
        try:
            line = line.split()
            glove_embeddings[line[0]] = np.array(line[1:], dtype=np.float32)
        except:
            continue
</code></pre>
<pre data-lang="python" class="language-python "><code class="language-python" data-lang="python">embed_len = 50

word_embeddings = np.zeros((len(tokenizer.index_word)+1, embed_len))

for idx, word in tokenizer.index_word.items():
    word_embeddings[idx] = glove_embeddings.get(word, np.zeros(embed_len))
</code></pre>
<pre data-lang="python" class="language-python "><code class="language-python" data-lang="python">word_embeddings[1][:10]
</code></pre>
<pre><code>array([ 0.15272   ,  0.36181   , -0.22168   ,  0.066051  ,  0.13029   ,
        0.37075001, -0.75874001, -0.44722   ,  0.22563   ,  0.10208   ])
</code></pre>
<h3 id="approach-1-glove-embeddings-flattened-max-tokens-50-embedding-length-300">Approach 1: GloVe Embeddings Flattened (Max Tokens=50, Embedding Length=300)</h3>
<h3 id="load-previously-trained-model">Load previously trained model</h3>
<pre data-lang="python" class="language-python "><code class="language-python" data-lang="python"># Load model
model_file = &#x27;models&#x2F;sparse_cat_entire&#x27;
model = keras.models.load_model(model_file)
model._name = &quot;SparseCategoricalEntireData&quot;
model.summary()
</code></pre>
<pre><code>Model: &quot;SparseCategoricalEntireData&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_2 (Embedding)     (None, 50, 50)            486750    
                                                                 
 flatten_2 (Flatten)         (None, 2500)              0         
                                                                 
 dense_6 (Dense)             (None, 128)               320128    
                                                                 
 dense_7 (Dense)             (None, 64)                8256      
                                                                 
 dense_8 (Dense)             (None, 7)                 455       
                                                                 
=================================================================
Total params: 815,589
Trainable params: 328,839
Non-trainable params: 486,750
_________________________________________________________________
</code></pre>
<pre data-lang="python" class="language-python "><code class="language-python" data-lang="python">model.weights[0][1][:10], word_embeddings[1][:10]
</code></pre>
<pre><code>(&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=
 array([ 0.15272 ,  0.36181 , -0.22168 ,  0.066051,  0.13029 ,  0.37075 ,
        -0.75874 , -0.44722 ,  0.22563 ,  0.10208 ], dtype=float32)&gt;,
 array([ 0.15272   ,  0.36181   , -0.22168   ,  0.066051  ,  0.13029   ,
         0.37075001, -0.75874001, -0.44722   ,  0.22563   ,  0.10208   ]))
</code></pre>
<pre data-lang="python" class="language-python "><code class="language-python" data-lang="python">model.fit(X_train_vect, y_train, batch_size=32, epochs=8, validation_data=(X_test_vect, y_test))
</code></pre>
<pre><code>Epoch 1&#x2F;8
883&#x2F;883 [==============================] - 6s 6ms&#x2F;step - loss: 0.6581 - accuracy: 0.8125 - val_loss: 0.5072 - val_accuracy: 0.8456
Epoch 2&#x2F;8
883&#x2F;883 [==============================] - 5s 6ms&#x2F;step - loss: 0.3699 - accuracy: 0.8839 - val_loss: 0.4601 - val_accuracy: 0.8626
Epoch 3&#x2F;8
883&#x2F;883 [==============================] - 5s 6ms&#x2F;step - loss: 0.2727 - accuracy: 0.9141 - val_loss: 0.4704 - val_accuracy: 0.8636
Epoch 4&#x2F;8
883&#x2F;883 [==============================] - 5s 6ms&#x2F;step - loss: 0.2132 - accuracy: 0.9316 - val_loss: 0.4715 - val_accuracy: 0.8747
Epoch 5&#x2F;8
883&#x2F;883 [==============================] - 5s 6ms&#x2F;step - loss: 0.1715 - accuracy: 0.9457 - val_loss: 0.4943 - val_accuracy: 0.8779
Epoch 6&#x2F;8
883&#x2F;883 [==============================] - 5s 6ms&#x2F;step - loss: 0.1461 - accuracy: 0.9529 - val_loss: 0.5230 - val_accuracy: 0.8780
Epoch 7&#x2F;8
883&#x2F;883 [==============================] - 6s 6ms&#x2F;step - loss: 0.1356 - accuracy: 0.9551 - val_loss: 0.5588 - val_accuracy: 0.8762
Epoch 8&#x2F;8
883&#x2F;883 [==============================] - 5s 6ms&#x2F;step - loss: 0.1132 - accuracy: 0.9634 - val_loss: 0.5902 - val_accuracy: 0.8782


&lt;keras.callbacks.History at 0x7f5a97e5e0a0&gt;
</code></pre>
<pre data-lang="python" class="language-python "><code class="language-python" data-lang="python">from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

y_preds = model.predict(X_test_vect).argmax(axis=-1)

print(&quot;Test Accuracy : {}&quot;.format(accuracy_score(y_test, y_preds)))
print(&quot;\nClassification Report : &quot;)
print(classification_report(y_test, y_preds, target_names=target_categories))
print(&quot;\nConfusion Matrix : &quot;)
print(confusion_matrix(y_test, y_preds))
</code></pre>
<pre><code>295&#x2F;295 [==============================] - 1s 2ms&#x2F;step
Test Accuracy : 0.8781731279872543

Classification Report : 
              precision    recall  f1-score   support

       sport       0.83      0.86      0.85       992
       autos       0.87      0.92      0.90      3428
    religion       0.89      0.89      0.89      1312
   comp_elec       0.91      0.89      0.90      1212
     sci_med       0.90      0.84      0.87       988
      seller       0.76      0.71      0.74       486
    politics       0.93      0.83      0.88       997

    accuracy                           0.88      9415
   macro avg       0.87      0.85      0.86      9415
weighted avg       0.88      0.88      0.88      9415


Confusion Matrix : 
[[ 857   70   14   11   15   12   13]
 [  70 3160   38   33   35   78   14]
 [  28   65 1165   36   10    3    5]
 [  13   61   41 1080    7    3    7]
 [  20   87   23   12  829    4   13]
 [  12  106    5    4    5  347    7]
 [  32   75   23   11   16   10  830]]
</code></pre>
<pre data-lang="python" class="language-python "><code class="language-python" data-lang="python"># TensorFlow SavedModel format =&gt; .keras
model_file = &#x27;models&#x2F;sparse_cat_entire&#x27;
model.save(model_file)
</code></pre>
<pre data-lang="python" class="language-python "><code class="language-python" data-lang="python">!pip install scikit-plot
from sklearn.metrics import confusion_matrix
import scikitplot as skplt
import matplotlib.pyplot as plt
import numpy as np

skplt.metrics.plot_confusion_matrix([target_categories[i] for i in y_test], [target_categories[i] for i in y_preds],
                                    normalize=True,
                                    title=&quot;Confusion Matrix&quot;,
                                    cmap=&quot;Greens&quot;,
                                    hide_zeros=True,
                                    figsize=(5,5)
                                    );
plt.xticks(rotation=90);
</code></pre>
<p><img src="/assets/images/glove/run_02.png#img-thumbnail" alt="png" /></p>
<h3 id="custom-test">Custom Test</h3>
<pre data-lang="python" class="language-python "><code class="language-python" data-lang="python"># define documents
docs = [&#x27;Democrats the Reuplicans are both the worst!&#x27;,
&#x27;Coyotes win 10-0&#x27;,
&#x27;Houston Astros defeat the Cubs&#x27;,
&#x27;Apple and Microsoft both make great computers&#x27;,
&#x27;New washer 4sale. $200&#x27;]

doc_array = np.array(docs)

doc_array_vect  = pad_sequences(tokenizer.texts_to_sequences(doc_array), maxlen=max_tokens, padding=&quot;post&quot;, truncating=&quot;post&quot;, value=0.)

cstm_test_preds = model.predict(doc_array_vect).argmax(axis=-1)
</code></pre>
<pre><code>1&#x2F;1 [==============================] - 0s 18ms&#x2F;step
</code></pre>
<pre data-lang="python" class="language-python "><code class="language-python" data-lang="python">print(doc_array)
</code></pre>
<pre><code>[&#x27;Democrats the Reuplicans are both the worst!&#x27; &#x27;Coyotes win 10-0&#x27;
 &#x27;Houston Astros defeat the Cubs&#x27;
 &#x27;Apple and Microsoft both make great computers&#x27; &#x27;New washer 4sale. $200&#x27;]
</code></pre>
<pre data-lang="python" class="language-python "><code class="language-python" data-lang="python">print(doc_array_vect)
</code></pre>
<pre><code>[[   1   48 2712    1  476    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0]
 [  52  465   65    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0]
 [7459 2362    1  988    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0]
 [ 138    3  105 2712  279  402 1625    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0]]
</code></pre>
<pre data-lang="python" class="language-python "><code class="language-python" data-lang="python">print(cstm_test_preds)
</code></pre>
<pre><code>[6 1 6 1 3]
</code></pre>


    </div>
  </section>
</body>

</html>